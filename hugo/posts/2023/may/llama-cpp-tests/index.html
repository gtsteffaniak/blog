<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Comparing some LLMs"><title>My experience benchmarking llama</title>
<link rel=canonical href=/blog/hugo/posts/2023/may/llama-cpp-tests/><link rel=stylesheet href=/blog/hugo/scss/style.min.017c4e0f16a1b4da2921a44c337e51d1e47cb6fe62902a184732583909e60b04.css><meta property="og:title" content="My experience benchmarking llama"><meta property="og:description" content="Comparing some LLMs"><meta property="og:url" content="/blog/hugo/posts/2023/may/llama-cpp-tests/"><meta property="og:site_name" content="Graham's Blog"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:tag" content="artificial-intelligence"><meta property="article:published_time" content="2023-05-14T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-14T00:00:00+00:00"><meta name=twitter:title content="My experience benchmarking llama"><meta name=twitter:description content="Comparing some LLMs"><link rel="shortcut icon" href=favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><nav class=top-navbar><div class="inner svelte-1y7m1qb"><button class=button-navbar style=background-color:#7d0e9e><div><svg class="ham hamRotate180 ham5 svelte-hzpm3z" viewBox="0 0 100 100" width="30"><path class="line top svelte-hzpm3z" d="m30 33h40s8.5-.68551 8.5 10.375c0 8.292653-6.122707 9.002293-8.5 6.625L58.928571 38.928571"/><path class="line middle svelte-hzpm3z" d="m70 50H30"/><path class="line bottom svelte-hzpm3z" d="m30 67h40s8.5.68551 8.5-10.375c0-8.292653-6.122707-9.002293-8.5-6.625L58.928571 61.071429"/></svg></div></button>
<button class=button-navbar style=background-color:#7d0e9e>
<a href=/><div style=width:100px;color:#fff>gPortal</div></a></button>
<button class=button-navbar style=background-color:#7d0e9e>
<a href=/login/><i class="mi mi-user-check svelte-1y7m1qb"></i></a></button></div></nav><div class="container main-container flex on-phone--column extended"><aside class="sidebar gcard left-sidebar"><div class=inner><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/hugo><img src=https://avatars.githubusercontent.com/u/42989099 width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/blog/hugo>Graham's Blog</a></h1><h2 class=site-description>My blog about technology</h2></div></header><ol class=social-menu><li><a href=https://github.com/gtsteffaniak target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><div class=schemas_listing><div class="list_item darkBackground" style=font-weight:700>2023<hr><div class="list_item darkBackground" style=font-weight:700>December<br><a class=post_link>Process to simplify a modules</a><a class=post_link>Happy
Holidays!</a></div><div class="list_item darkBackground" style=font-weight:700>October<br><a class=post_link>The doubled edge sword of modules when developing</a></div><div class="list_item darkBackground" style=font-weight:700>September<br><a class=post_link>Comparing the top 3 compiled low-level languages in 2023</a></div><div class="list_item darkBackground" style=font-weight:700>May<br><a class=post_link>Testing LLAMA AI chat versions</a><a class=post_link>May the 4th be
with you</a></div><div class="list_item darkBackground" style=font-weight:700>April<br><a class=post_link>Bash vs. Powershell</a><a class=post_link>It's My
First Post!</a></div></div></div><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></div></aside><aside class="sidebar gcard right-sidebar sticky"><div class=inner><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#llamacpp>llama.cpp</a></li><li><a href=#go-llama>Go llama</a></li><li><a href=#bonus-round>Bonus round</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></div></aside><main class="main gcard full-width"><div class=inner><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/blog/hugo/posts/2023/may/llama-cpp-tests/>My experience benchmarking llama</a></h2><h3 class=article-subtitle>Comparing some LLMs</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>May 14, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>14 minute read</time></div></footer><header class=article-category><p class=article-subtitle>Categories:</p><a href=/blog/hugo/categories/technology/>technology</a></header></div></header><section class=article-content><hr style=width:100%><p>Today, I had the opportunity to benchmark a fascinating program called &ldquo;llama.cpp&rdquo; that has been ported to work with multiple programming languages, including Python and Golang. As an enthusiast of both Python and Golang, I was particularly interested in comparing the performance of these two implementations on my M1 Arm64 MacBook.</p><p>In this benchmark, I tested three different implementations:</p><ul><li><a class=link href=https://github.com/ggerganov/llama.cpp target=_blank rel=noopener>llama.cpp</a></li><li><a class=link href=https://github.com/gotzmann/llama.go target=_blank rel=noopener>llama.go</a></li></ul><p>Naturally, you might be curious about which implementation performed the fastest. It&rsquo;s worth noting that the native version of llama.cpp is likely to have the advantage in terms of speed. This advantage stems not from the inherent speed of the programming language but rather from the fact that it is the &ldquo;upstream&rdquo; branch that receives all the changes and performance optimizations first. Consequently, the Python and Golang versions may not have benefited from these optimizations yet.</p><h2 id=llamacpp>llama.cpp</h2><p>execution:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>./</span><span class=n>main</span> <span class=o>-</span><span class=n>m</span> <span class=o>~/</span><span class=n>models</span><span class=o>/</span><span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span> <span class=o>-</span><span class=n>c</span> <span class=mi>45</span> <span class=o>-</span><span class=n>n</span> <span class=mi>45</span>
</span></span><span class=line><span class=cl><span class=n>main</span><span class=p>:</span> <span class=n>build</span> <span class=o>=</span> <span class=mi>548</span> <span class=p>(</span><span class=mi>60</span><span class=n>f8c36</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>main</span><span class=p>:</span> <span class=nb>seed</span>  <span class=o>=</span> <span class=mi>1684077400</span>
</span></span><span class=line><span class=cl><span class=n>llama</span><span class=o>.</span><span class=n>cpp</span><span class=p>:</span> <span class=n>loading</span> <span class=n>model</span> <span class=n>from</span> <span class=o>/</span><span class=n>Users</span><span class=o>/</span><span class=n>steffag</span><span class=o>/</span><span class=n>models</span><span class=o>/</span><span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>format</span>     <span class=o>=</span> <span class=n>ggjt</span> <span class=n>v1</span> <span class=p>(</span><span class=n>pre</span> <span class=c1>#1405)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_vocab</span>    <span class=o>=</span> <span class=mi>32000</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ctx</span>      <span class=o>=</span> <span class=mi>45</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_embd</span>     <span class=o>=</span> <span class=mi>4096</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_mult</span>     <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_head</span>     <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_layer</span>    <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_rot</span>      <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ftype</span>      <span class=o>=</span> <span class=mi>0</span> <span class=p>(</span><span class=n>all</span> <span class=n>F32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ff</span>       <span class=o>=</span> <span class=mi>11008</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_parts</span>    <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>model</span> <span class=n>size</span> <span class=o>=</span> <span class=mi>7</span><span class=n>B</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ggml</span> <span class=n>ctx</span> <span class=n>size</span> <span class=o>=</span>  <span class=mf>72.75</span> <span class=n>KB</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>mem</span> <span class=n>required</span>  <span class=o>=</span> <span class=mf>27497.09</span> <span class=n>MB</span> <span class=p>(</span><span class=o>+</span> <span class=mf>1026.00</span> <span class=n>MB</span> <span class=n>per</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_init_from_file</span><span class=p>:</span> <span class=n>kv</span> <span class=bp>self</span> <span class=n>size</span>  <span class=o>=</span>   <span class=mf>22.50</span> <span class=n>MB</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>system_info</span><span class=p>:</span> <span class=n>n_threads</span> <span class=o>=</span> <span class=mi>8</span> <span class=o>/</span> <span class=mi>10</span> <span class=o>|</span> <span class=n>AVX</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX2</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX512</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX512_VBMI</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX512_VNNI</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>FMA</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>NEON</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>ARM_FMA</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>F16C</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>FP16_VA</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>WASM_SIMD</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>BLAS</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>SSE3</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>VSX</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl><span class=n>sampling</span><span class=p>:</span> <span class=n>repeat_last_n</span> <span class=o>=</span> <span class=mi>64</span><span class=p>,</span> <span class=n>repeat_penalty</span> <span class=o>=</span> <span class=mf>1.100000</span><span class=p>,</span> <span class=n>presence_penalty</span> <span class=o>=</span> <span class=mf>0.000000</span><span class=p>,</span> <span class=n>frequency_penalty</span> <span class=o>=</span> <span class=mf>0.000000</span><span class=p>,</span> <span class=n>top_k</span> <span class=o>=</span> <span class=mi>40</span><span class=p>,</span> <span class=n>tfs_z</span> <span class=o>=</span> <span class=mf>1.000000</span><span class=p>,</span> <span class=n>top_p</span> <span class=o>=</span> <span class=mf>0.950000</span><span class=p>,</span> <span class=n>typical_p</span> <span class=o>=</span> <span class=mf>1.000000</span><span class=p>,</span> <span class=n>temp</span> <span class=o>=</span> <span class=mf>0.800000</span><span class=p>,</span> <span class=n>mirostat</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>mirostat_lr</span> <span class=o>=</span> <span class=mf>0.100000</span><span class=p>,</span> <span class=n>mirostat_ent</span> <span class=o>=</span> <span class=mf>5.000000</span>
</span></span><span class=line><span class=cl><span class=n>generate</span><span class=p>:</span> <span class=n>n_ctx</span> <span class=o>=</span> <span class=mi>45</span><span class=p>,</span> <span class=n>n_batch</span> <span class=o>=</span> <span class=mi>512</span><span class=p>,</span> <span class=n>n_predict</span> <span class=o>=</span> <span class=mi>45</span><span class=p>,</span> <span class=n>n_keep</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=err>←</span> <span class=n>The</span> <span class=n>Forgotten</span> <span class=n>Story</span> <span class=n>of</span> <span class=n>the</span> <span class=n>First</span> <span class=n>Civil</span> <span class=n>War</span> <span class=n>Battle</span> <span class=ow>in</span> <span class=n>Kansas</span>
</span></span><span class=line><span class=cl><span class=n>Making</span> <span class=n>It</span> <span class=n>Home</span> <span class=n>from</span> <span class=n>the</span> <span class=n>Front</span> <span class=err>→</span>
</span></span><span class=line><span class=cl><span class=n>I</span> <span class=n>Have</span> <span class=n>a</span> <span class=n>Dream</span><span class=err>—</span><span class=n>That</span> <span class=n>We</span> <span class=n>Finally</span> <span class=n>Learn</span> <span class=n>More</span> <span class=n>About</span> <span class=n>Frederick</span> <span class=n>Douglass</span><span class=o>!</span>
</span></span><span class=line><span class=cl><span class=err>“</span><span class=n>I</span> <span class=n>have</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=nb>load</span> <span class=n>time</span> <span class=o>=</span>  <span class=mf>9196.58</span> <span class=n>ms</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>      <span class=n>sample</span> <span class=n>time</span> <span class=o>=</span>    <span class=mf>22.49</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>45</span> <span class=n>runs</span>   <span class=p>(</span>    <span class=mf>0.50</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span> <span class=n>prompt</span> <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>10716.26</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>25</span> <span class=n>tokens</span> <span class=p>(</span>  <span class=mf>428.65</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>11689.89</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>43</span> <span class=n>runs</span>   <span class=p>(</span>  <span class=mf>271.86</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>       <span class=n>total</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>22483.24</span> <span class=n>ms</span>
</span></span></code></pre></td></tr></table></div></div><p>Memory usage : 35 MB
CPU usage : 85%</p><p>OK! Easy enough. It took 11 seconds to print, with <strong>272 ms per token</strong>!</p><h2 id=go-llama>Go llama</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>LIBRARY_PATH</span><span class=o>=$</span><span class=n>PWD</span> <span class=n>C_INCLUDE_PATH</span><span class=o>=$</span><span class=n>PWD</span> <span class=n>go</span> <span class=n>run</span> <span class=o>./</span><span class=n>examples</span> <span class=o>-</span><span class=n>m</span> <span class=o>~/</span><span class=n>models</span><span class=o>/</span><span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span> <span class=o>-</span><span class=n>n</span> <span class=mi>45</span>      
</span></span><span class=line><span class=cl><span class=n>llama</span><span class=o>.</span><span class=n>cpp</span><span class=p>:</span> <span class=n>loading</span> <span class=n>model</span> <span class=n>from</span> <span class=o>/</span><span class=n>Users</span><span class=o>/</span><span class=n>steffag</span><span class=o>/</span><span class=n>models</span><span class=o>/</span><span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>format</span>     <span class=o>=</span> <span class=n>ggjt</span> <span class=n>v1</span> <span class=p>(</span><span class=n>pre</span> <span class=c1>#1405)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_vocab</span>    <span class=o>=</span> <span class=mi>32000</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ctx</span>      <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_embd</span>     <span class=o>=</span> <span class=mi>4096</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_mult</span>     <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_head</span>     <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_layer</span>    <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_rot</span>      <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ftype</span>      <span class=o>=</span> <span class=mi>0</span> <span class=p>(</span><span class=n>all</span> <span class=n>F32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ff</span>       <span class=o>=</span> <span class=mi>11008</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_parts</span>    <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>model</span> <span class=n>size</span> <span class=o>=</span> <span class=mi>7</span><span class=n>B</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ggml</span> <span class=n>ctx</span> <span class=n>size</span> <span class=o>=</span>  <span class=mf>68.20</span> <span class=n>KB</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>mem</span> <span class=n>required</span>  <span class=o>=</span> <span class=mf>27497.08</span> <span class=n>MB</span> <span class=p>(</span><span class=o>+</span> <span class=mf>2052.00</span> <span class=n>MB</span> <span class=n>per</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_init_from_file</span><span class=p>:</span> <span class=n>kv</span> <span class=bp>self</span> <span class=n>size</span>  <span class=o>=</span>  <span class=mf>128.00</span> <span class=n>MB</span>
</span></span><span class=line><span class=cl><span class=n>The</span> <span class=n>model</span> <span class=n>loaded</span> <span class=n>successfully</span><span class=o>.</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>What</span> <span class=n>is</span> <span class=n>the</span> <span class=n>fastest</span> <span class=n>programming</span> <span class=n>language</span><span class=err>?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Sending</span> <span class=n>what</span> <span class=n>is</span> <span class=n>the</span> <span class=n>fastest</span> <span class=n>programming</span> <span class=n>language</span><span class=err>?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>by</span> <span class=n>Cary</span> <span class=n>R</span> <span class=n>on</span> <span class=n>Jul</span> <span class=mi>18</span><span class=p>,</span> <span class=mi>2017</span><span class=p>,</span> <span class=n>at</span> <span class=mi>6</span><span class=p>:</span><span class=mi>45</span> <span class=n>UTC</span>
</span></span><span class=line><span class=cl><span class=n>what</span> <span class=k>do</span> <span class=n>you</span> <span class=n>think</span> <span class=n>it</span> <span class=n>is</span> <span class=ow>and</span> <span class=n>why</span><span class=err>?</span>
</span></span><span class=line><span class=cl><span class=n>I</span><span class=s1>&#39;m not sure what &#34;fast&#34; means for this</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=nb>load</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>99393.39</span> <span class=n>ms</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>      <span class=n>sample</span> <span class=n>time</span> <span class=o>=</span>    <span class=mf>32.26</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>45</span> <span class=n>runs</span>   <span class=p>(</span>    <span class=mf>0.72</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span> <span class=n>prompt</span> <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span>  <span class=mf>5021.80</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>10</span> <span class=n>tokens</span> <span class=p>(</span>  <span class=mf>502.18</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>15193.51</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>44</span> <span class=n>runs</span>   <span class=p>(</span>  <span class=mf>345.31</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>       <span class=n>total</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>115311.61</span> <span class=n>ms</span>
</span></span><span class=line><span class=cl><span class=n>Embeddings</span><span class=p>:</span> <span class=p>[</span><span class=mf>1.3335894</span> <span class=o>-</span><span class=mf>0.83280444</span> <span class=mf>0.9414267</span> <span class=o>-</span><span class=mf>9.215284</span> <span class=o>-</span><span class=mf>1.0302917</span> <span class=mf>1.065452</span> <span class=o>-</span><span class=mf>0.4542901</span> <span class=o>-</span><span class=mf>0.24896632</span> <span class=o>-</span><span class=mf>0.6570409</span> <span class=mf>1.9119468</span> <span class=mf>0.6292349</span> <span class=o>-</span><span class=mf>0.14391524</span> <span class=mf>0.2595427</span> <span class=o>-</span><span class=mf>0.5855895</span> <span class=o>-</span><span class=mf>0.963376</span> <span class=mf>1.0406973</span> <span class=o>-</span><span class=mf>0.1605502</span> <span class=mf>1.3280734</span> <span class=mf>0.37920082</span> <span class=mf>0.61060756</span> <span class=o>-</span><span class=mf>1.2766573</span> <span class=o>-</span><span class=mf>1.8673204</span> <span class=mf>1.2690753</span> <span class=o>-</span><span class=mf>0.4294657</span> <span class=mf>0.5546539</span> <span class=mf>0.11715727</span> <span class=mf>0.6430202</span> <span class=o>-</span><span class=mf>0.09789314</span> <span class=o>-</span><span class=mf>0.45095867</span> <span class=o>-</span><span class=mf>1.1076287</span> <span class=mf>0.042604066</span> <span class=mf>0.15544033</span> <span class=o>-</span><span class=mf>0.09977249</span> <span class=o>-</span><span class=mf>1.3832492</span> <span class=mf>0.018180523</span> <span class=mf>2.2709634</span> <span class=mf>0.26105422</span> <span class=o>-</span><span class=mf>1.0794421</span> <span class=mf>0.28251836</span> <span class=o>-</span><span class=mf>1.2772827</span> <span class=mf>1.3353819</span> <span class=o>-</span><span class=mf>1.1416842</span> <span class=mf>1.8800831</span> <span class=mf>0.7737296</span> <span class=mf>0.8329498</span> <span class=o>-</span><span class=mf>1.1428409</span> <span class=o>-</span><span class=mf>0.27773026</span> <span class=mf>0.59615296</span> <span class=o>-</span><span class=mf>1.1754322</span> <span class=o>-</span><span class=mf>0.61925936</span> <span class=mf>0.12707934</span> <span class=mf>0.33790576</span> <span class=mf>0.9590525</span> <span class=o>-</span><span class=mf>1.0039365</span> <span class=mf>1.2138838</span> <span class=o>-</span><span class=mf>0.15244572</span> <span class=mf>1.3892341</span> <span class=o>-</span><span class=mf>0.2408304</span> <span class=o>-</span><span class=mf>0.41973415</span> <span class=o>-</span><span class=mf>0.9122008</span> <span class=mf>0.61534476</span> <span class=o>-</span><span class=mf>1.3473209</span> <span class=mf>1.8957467</span> <span class=mf>0.54428715</span> <span class=o>-</span><span class=mf>0.45334002</span> <span class=o>-</span><span class=mf>0.46586785</span> <span class=mf>0.9365548</span> <span class=mf>0.7735351</span> <span class=mf>0.020367475</span> <span class=mf>0.03640651</span> <span class=mf>0.6072077</span> <span class=mf>0.2598248</span> <span class=o>-</span><span class=mf>0.60497457</span> <span class=mf>0.74164164</span> <span class=o>-</span><span class=mf>1.4986299</span> <span class=mf>0.030030286</span> <span class=mf>1.0310581</span> <span class=o>-</span><span class=mf>0.7985864</span> <span class=mf>0.59369475</span> <span class=mf>5.3009334</span> <span class=o>-</span><span class=mf>0.26436043</span> <span class=o>-</span><span class=mf>1.0086688</span> <span class=mf>0.69724923</span> <span class=o>-</span><span class=mf>0.082101144</span> <span class=mf>0.609409</span> <span class=o>-</span><span class=mf>0.4504542</span> <span class=o>-</span><span class=mf>0.57361007</span> <span class=o>-</span><span class=mf>0.43234673</span> <span class=o>-</span><span class=mf>0.621053</span> <span class=o>-</span><span class=mf>1.3142335</span> <span class=o>-</span><span class=mf>1.2885888</span> <span class=o>-</span><span class=mf>0.29704484</span> <span class=mf>0.16729134</span> <span class=o>-</span><span class=mf>0.76317424</span> <span class=mf>1.2080128</span> <span class=mf>0.24425012</span> <span class=o>-</span><span class=mf>0.3169634</span> <span class=mf>0.9270621</span> <span class=mf>1.0773871</span> <span class=o>-</span><span class=mf>0.09211676</span> <span class=mf>4.2189116</span> <span class=mf>1.1267253</span> <span class=o>-</span><span class=mf>1.2751623</span> <span class=o>-</span><span class=mf>0.04176733</span> <span class=o>-</span><span class=mf>1.0876625</span> <span class=o>-</span><span class=mf>0.19441187</span> <span class=mf>0.6124146</span> <span class=o>-</span><span class=mf>0.5224489</span> <span class=o>-</span><span class=mf>1.346519</span> <span class=o>-</span><span class=mf>0.129513</span> <span class=o>-</span><span class=mf>0.12585206</span> <span class=mf>0.9263705</span> <span class=o>-</span><span class=mf>1.6089619</span> <span class=o>-</span><span class=mf>1.5251873</span> <span class=mf>1.0640423</span> <span class=mf>1.1027105</span> <span class=o>-</span><span class=mf>0.5490974</span> <span class=o>-</span><span class=mf>0.85569364</span> <span class=o>-</span><span class=mf>1.1080054</span> <span class=mf>0.9023686</span> <span class=o>-</span><span class=mf>1.0494307</span> <span class=o>-</span><span class=mf>0.28588632</span> <span class=o>-</span><span class=mf>0.4288576</span> <span class=o>-</span><span class=mf>0.72663045</span> <span class=mf>1.7789608</span> <span class=mf>2.239715</span> <span class=mf>0.8199781</span> <span class=mf>0.4134441</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>Memory usage : 110 MB
CPU usage : 100%</p><p>Ok we have something - <strong>345 ms per token</strong>. Makes sense, since it is an indirect non-native form of calling what is the first test of cpp&mldr; except as a shared library via Golang. So what about natives?</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>./llama.go --model ~/models/llama-7b-fp32.bin --prompt &#34;Which programming language is fastest?&#34; --context 45 --predict 45 
</span></span><span class=line><span class=cl>                                                    
</span></span><span class=line><span class=cl>  /▒▒       /▒▒         /▒▒▒/▒▒▒   /▒▒/▒▒▒▒/▒▒   /▒▒▒/▒▒▒      /▒▒▒▒/▒▒   /▒▒▒/▒▒▒    
</span></span><span class=line><span class=cl>  /▒▒▒      /▒▒▒      /▒▒▒/ /▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒/ /▒▒▒     /▒▒▒▒ //   /▒▒▒▒//▒▒▒  
</span></span><span class=line><span class=cl>  /▒▒▒▒/▒▒  /▒▒▒▒/▒▒  /▒▒▒▒/▒▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒▒ /▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒ /▒▒▒▒ 
</span></span><span class=line><span class=cl>  /▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒ /▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒▒▒/▒▒  //▒▒▒/▒▒▒
</span></span><span class=line><span class=cl>  //// ///  //// ///  ///  ////  ///  //  ///  ///  ////  ///  //// //    /// ///
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   ▒▒▒▒ [ LLaMA.go v1.4.0 ] [ LLaMA GPT in pure Golang - based on LLaMA C++ ] ▒▒▒▒
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ INIT ] Loading model, please wait .............................
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ PROMPT ] Which programming language is the fastest?
</span></span><span class=line><span class=cl>[ OUTPUT ]
</span></span><span class=line><span class=cl> Which programming language is best for a particular task?
</span></span><span class=line><span class=cl> Which programming language should I use to write my program?
</span></span><span class=line><span class=cl> What are the advantages and disadvantages of using a particular programming language?
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Comment: @
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== EVAL TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>15247 | 3125 | 1742 | 3636 | 1253 | 1276 | 1420 | 1487 | 1394 | 1628 | 1659 | 5330 | 10200 | 5287 | 3114 | 1320 | 1560 | 1725 | 5190 | 1573 | 1342 | 1440 | 1365 | 3922 | 13103 | 12436 | 3441 | 1597 | 1598 | 1537 | 1487 | 8702 | 3437 | 1780 | 10625 | 15753 | 1869 | 29020 | 7898 | 1574 | 1655 | 1664 | 1652 | 1604 | 1483 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== SAMPLING TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>7 | 7 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 8 | 7 | 7 | 7 | 7 | 8 | 7 | 7 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 7 | 7 | 7 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== FULL TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>0 | 15255 | 3132 | 1751 | 3643 | 1260 | 1283 | 1427 | 1495 | 1402 | 1635 | 1667 | 5339 | 10208 | 5295 | 3121 | 1328 | 1568 | 1732 | 5198 | 1581 | 1350 | 1448 | 1372 | 3930 | 13111 | 12444 | 3449 | 1605 | 1606 | 1545 | 1495 | 8710 | 3445 | 1788 | 10633 | 15761 | 1876 | 29028 | 7906 | 1581 | 1663 | 1673 | 1660 | 1612 | 1490 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ HALT ] Time per token: 4380 ms | Tokens per second: 0.23
</span></span></code></pre></td></tr></table></div></div><p>Memory usage : 25 GB
CPU usage : 85%</p><p>Looks like it loaded the entire model into RAM! ok&mldr; well does that translate into better performance? Not good. I can tell you it <em>felt</em> slow, 4380ms seems &mldr; accurate. Hmm&mldr; maybe I should use the neon flag for my m1.</p><p>Testing neon first:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>./llama.go --model ~/models/llama-7b-fp32.bin --prompt &#34;Which programming language is fastest?&#34; --neon --context 45 --predict 45
</span></span><span class=line><span class=cl>                                                    
</span></span><span class=line><span class=cl>  /▒▒       /▒▒         /▒▒▒/▒▒▒   /▒▒/▒▒▒▒/▒▒   /▒▒▒/▒▒▒      /▒▒▒▒/▒▒   /▒▒▒/▒▒▒    
</span></span><span class=line><span class=cl>  /▒▒▒      /▒▒▒      /▒▒▒/ /▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒/ /▒▒▒     /▒▒▒▒ //   /▒▒▒▒//▒▒▒  
</span></span><span class=line><span class=cl>  /▒▒▒▒/▒▒  /▒▒▒▒/▒▒  /▒▒▒▒/▒▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒▒ /▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒ /▒▒▒▒ 
</span></span><span class=line><span class=cl>  /▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒ /▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒▒▒/▒▒  //▒▒▒/▒▒▒
</span></span><span class=line><span class=cl>  //// ///  //// ///  ///  ////  ///  //  ///  ///  ////  ///  //// //    /// ///
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   ▒▒▒▒ [ LLaMA.go v1.4.0 ] [ LLaMA GPT in pure Golang - based on LLaMA C++ ] ▒▒▒▒
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ INIT ] Loading model, please wait .............................
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ PROMPT ] Which programming language is the fastest?
</span></span><span class=line><span class=cl>[ OUTPUT ]
</span></span><span class=line><span class=cl> Which programming language is easiest to use?
</span></span><span class=line><span class=cl> Which programming language is the best for beginners?
</span></span><span class=line><span class=cl> Which programming language is the best for experienced programmers?
</span></span><span class=line><span class=cl> Which programming language has the most potential?
</span></span><span class=line><span class=cl> Which programming
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== EVAL TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>12559 | 3044 | 310 | 307 | 355 | 360 | 309 | 324 | 305 | 318 | 324 | 329 | 328 | 313 | 306 | 319 | 315 | 317 | 314 | 320 | 331 | 315 | 321 | 313 | 314 | 324 | 335 | 342 | 325 | 312 | 321 | 315 | 309 | 330 | 307 | 324 | 343 | 5317 | 312 | 312 | 307 | 319 | 323 | 316 | 309 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== SAMPLING TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== FULL TIME | ms ===
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>0 | 12567 | 3051 | 318 | 315 | 363 | 367 | 316 | 331 | 313 | 326 | 331 | 337 | 336 | 321 | 313 | 326 | 323 | 325 | 321 | 328 | 339 | 322 | 328 | 321 | 322 | 331 | 342 | 350 | 333 | 319 | 329 | 323 | 316 | 338 | 314 | 332 | 350 | 5325 | 320 | 320 | 315 | 327 | 331 | 324 | 316 | 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[ HALT ] Time per token: 754 ms | Tokens per second: 1.33
</span></span></code></pre></td></tr></table></div></div><p>Memory usage : 25GB
CPU usage : 66%</p><p>The execution time significantly improved with the latest changes! The reported duration stands at approximately <strong>754 ms per token</strong>. However, upon closer examination, it becomes evident that the initial token took a whopping 40 times longer to process, while the second token took 10 times longer compared to the rest. This disparity skews the average calculation, and a more realistic estimate would be around <strong>315 ms per token</strong>, which aligns better with the perceived speed.</p><p>Nonetheless, it is worth noting that the program still has room for further optimization, as it did not fully utilize the available CPU resources. It is disappointing to observe that the current implementation falls short in this regard. Additionally, the necessity to load the model into RAM remains a drawback, especially when compared to the native C++ version, which accomplishes the same task with minimal RAM usage and greater efficiency.</p><p>While the recent improvements have led to a noticeable boost in performance, there is still potential for even better optimizations. The requirement to load the model into RAM remains a limitation that hampers efficiency, particularly when compared to the native C++ version&rsquo;s streamlined approach.</p><p>Lastly, is there any way we can use 100% of the CPU and squeeze out better performance? let&rsquo;s try.</p><ol><li>using <code>--neon --context 45 --predict 45 --threads 10 --silent --profile 2023/05/14 10:25:56 profile: CPU profiling enabled, cpu.pprof</code> I get 65% usage with 675 ms.</li><li>20 threads get 772ms</li><li>6 gave 50% usage with 745 ms per token</li><li>4 gave 35% usage with 915 ms per token</li><li>2 gave 20% usage with 1608 ms per token</li></ol><p>During the course of testing, it became evident that all the runs exhibited a similar pattern. The initial two tokens took an exceptionally long time to process, but subsequently, the performance noticeably improved. In a hypothetical scenario where superior optimizations were implemented, it is plausible that the native Go version could outperform the native C++ version, particularly if it efficiently utilized all CPU threads through goroutines. However, it must be acknowledged that the current implementation falls short in terms of performance. Additionally, one notable disadvantage of the native Go version is its inability to load the model in smaller segments, as the native C++ version does, thereby avoiding the excessive consumption of 25GB of RAM.</p><p>In summary, although the native Go version has the potential for faster performance through effective CPU thread utilization, it currently lags behind due to performance limitations. Furthermore, it lacks the advantageous feature present in the native C++ version of loading the model in smaller, more memory-efficient chunks.</p><p>Ok, that&rsquo;s enough of that.</p><h2 id=bonus-round>Bonus round</h2><p>The llama.cpp repo released GPU support for the program - so I had to try that out too. I don&rsquo;t have anything too beefy to test it on , but I have a 1050 GPU laptop. So I went there and tested with GPU support enabled:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>.</span>\<span class=n>main</span><span class=o>.</span><span class=n>exe</span> <span class=o>-</span><span class=n>m</span> <span class=n>C</span><span class=p>:</span>\<span class=n>Users</span>\<span class=n>graha</span>\<span class=n>OneDrive</span>\<span class=n>Desktop</span>\<span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span> <span class=o>-</span><span class=n>p</span> <span class=s2>&#34;what is a banana doing on my lawn?&#34;</span> <span class=o>-</span><span class=n>t</span> <span class=mi>12</span> <span class=o>-</span><span class=n>ngl</span> <span class=mi>4</span> <span class=o>-</span><span class=n>c</span> <span class=mi>45</span> <span class=o>-</span><span class=n>n</span> <span class=mi>45</span>
</span></span><span class=line><span class=cl><span class=n>main</span><span class=p>:</span> <span class=n>build</span> <span class=o>=</span> <span class=mi>550</span> <span class=p>(</span><span class=mi>79</span><span class=n>b2d5b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>main</span><span class=p>:</span> <span class=nb>seed</span>  <span class=o>=</span> <span class=mi>1684092347</span>
</span></span><span class=line><span class=cl><span class=n>llama</span><span class=o>.</span><span class=n>cpp</span><span class=p>:</span> <span class=n>loading</span> <span class=n>model</span> <span class=n>from</span> <span class=n>C</span><span class=p>:</span>\<span class=n>Users</span>\<span class=n>graha</span>\<span class=n>OneDrive</span>\<span class=n>Desktop</span>\<span class=n>llama</span><span class=o>-</span><span class=mi>7</span><span class=n>b</span><span class=o>-</span><span class=n>fp32</span><span class=o>.</span><span class=n>bin</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>format</span>     <span class=o>=</span> <span class=n>ggjt</span> <span class=n>v1</span> <span class=p>(</span><span class=n>pre</span> <span class=c1>#1405)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_vocab</span>    <span class=o>=</span> <span class=mi>32000</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ctx</span>      <span class=o>=</span> <span class=mi>45</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_embd</span>     <span class=o>=</span> <span class=mi>4096</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_mult</span>     <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_head</span>     <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_layer</span>    <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_rot</span>      <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ftype</span>      <span class=o>=</span> <span class=mi>0</span> <span class=p>(</span><span class=n>all</span> <span class=n>F32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_ff</span>       <span class=o>=</span> <span class=mi>11008</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>n_parts</span>    <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>model</span> <span class=n>size</span> <span class=o>=</span> <span class=mi>7</span><span class=n>B</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>ggml</span> <span class=n>ctx</span> <span class=n>size</span> <span class=o>=</span>  <span class=mf>72.75</span> <span class=n>KB</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=n>mem</span> <span class=n>required</span>  <span class=o>=</span> <span class=mf>27497.09</span> <span class=n>MB</span> <span class=p>(</span><span class=o>+</span> <span class=mf>1026.00</span> <span class=n>MB</span> <span class=n>per</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=p>[</span><span class=n>cublas</span><span class=p>]</span> <span class=n>offloading</span> <span class=mi>4</span> <span class=n>layers</span> <span class=n>to</span> <span class=n>GPU</span>
</span></span><span class=line><span class=cl><span class=n>llama_model_load_internal</span><span class=p>:</span> <span class=p>[</span><span class=n>cublas</span><span class=p>]</span> <span class=n>total</span> <span class=n>VRAM</span> <span class=n>used</span><span class=p>:</span> <span class=mi>3088</span> <span class=n>MB</span>
</span></span><span class=line><span class=cl><span class=n>llama_init_from_file</span><span class=p>:</span> <span class=n>kv</span> <span class=bp>self</span> <span class=n>size</span>  <span class=o>=</span>   <span class=mf>22.50</span> <span class=n>MB</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>system_info</span><span class=p>:</span> <span class=n>n_threads</span> <span class=o>=</span> <span class=mi>12</span> <span class=o>/</span> <span class=mi>12</span> <span class=o>|</span> <span class=n>AVX</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>AVX2</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>AVX512</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX512_VBMI</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>AVX512_VNNI</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>FMA</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>NEON</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>ARM_FMA</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>F16C</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>FP16_VA</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>WASM_SIMD</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>BLAS</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>SSE3</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>|</span> <span class=n>VSX</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=n>sampling</span><span class=p>:</span> <span class=n>repeat_last_n</span> <span class=o>=</span> <span class=mi>64</span><span class=p>,</span> <span class=n>repeat_penalty</span> <span class=o>=</span> <span class=mf>1.100000</span><span class=p>,</span> <span class=n>presence_penalty</span> <span class=o>=</span> <span class=mf>0.000000</span><span class=p>,</span> <span class=n>frequency_penalty</span> <span class=o>=</span> <span class=mf>0.000000</span><span class=p>,</span> <span class=n>top_k</span> <span class=o>=</span> <span class=mi>40</span><span class=p>,</span> <span class=n>tfs_z</span> <span class=o>=</span> <span class=mf>1.000000</span><span class=p>,</span> <span class=n>top_p</span> <span class=o>=</span> <span class=mf>0.950000</span><span class=p>,</span> <span class=n>typical_p</span> <span class=o>=</span> <span class=mf>1.000000</span><span class=p>,</span> <span class=n>temp</span> <span class=o>=</span> <span class=mf>0.800000</span><span class=p>,</span> <span class=n>mirostat</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>mirostat_lr</span> <span class=o>=</span> <span class=mf>0.100000</span><span class=p>,</span> <span class=n>mirostat_ent</span> <span class=o>=</span> <span class=mf>5.000000</span>
</span></span><span class=line><span class=cl><span class=n>generate</span><span class=p>:</span> <span class=n>n_ctx</span> <span class=o>=</span> <span class=mi>45</span><span class=p>,</span> <span class=n>n_batch</span> <span class=o>=</span> <span class=mi>512</span><span class=p>,</span> <span class=n>n_predict</span> <span class=o>=</span> <span class=mi>45</span><span class=p>,</span> <span class=n>n_keep</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=n>what</span> <span class=n>is</span> <span class=n>a</span> <span class=n>banana</span> <span class=n>doing</span> <span class=n>on</span> <span class=n>my</span> <span class=n>lawn</span><span class=err>?</span>
</span></span><span class=line><span class=cl><span class=n>the</span> <span class=n>other</span> <span class=n>day</span> <span class=n>i</span> <span class=n>was</span> <span class=n>looking</span> <span class=n>out</span> <span class=n>the</span> <span class=n>window</span> <span class=ow>and</span> <span class=n>saw</span> <span class=n>a</span> <span class=n>banana</span> <span class=n>sitting</span> <span class=ow>in</span> <span class=n>our</span> <span class=n>front</span> <span class=n>yard</span><span class=o>.</span> <span class=n>i</span> <span class=n>walked</span> <span class=n>outside</span> <span class=n>to</span> <span class=n>see</span> <span class=k>if</span> <span class=n>anyone</span> <span class=n>had</span> <span class=n>left</span> <span class=n>it</span> <span class=n>there</span> <span class=n>by</span> <span class=n>accident</span><span class=p>,</span> <span class=n>but</span> <span class=n>no</span> <span class=n>one</span> <span class=n>answered</span> <span class=n>when</span> <span class=n>i</span> <span class=n>called</span> <span class=k>for</span> <span class=n>them</span> <span class=n>inside</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=nb>load</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>35609.62</span> <span class=n>ms</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>      <span class=n>sample</span> <span class=n>time</span> <span class=o>=</span>    <span class=mf>10.52</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>45</span> <span class=n>runs</span>   <span class=p>(</span>    <span class=mf>0.23</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span> <span class=n>prompt</span> <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>31931.20</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>35</span> <span class=n>tokens</span> <span class=p>(</span>  <span class=mf>912.32</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>        <span class=n>eval</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>36327.48</span> <span class=n>ms</span> <span class=o>/</span>    <span class=mi>43</span> <span class=n>runs</span>   <span class=p>(</span>  <span class=mf>844.83</span> <span class=n>ms</span> <span class=n>per</span> <span class=n>token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llama_print_timings</span><span class=p>:</span>       <span class=n>total</span> <span class=n>time</span> <span class=o>=</span> <span class=mf>74111.62</span> <span class=n>ms</span>
</span></span></code></pre></td></tr></table></div></div><p>CPU usage : 100%
memory usage : 25GB</p><p>Hmm, okay, so GPU support doesn&rsquo;t provide significant benefits apart from offloading some of the RAM usage, resulting in a token processing time of approximately 844 ms (which is similar to the non-GPU version). Interestingly, the MacBook optimized code doesn&rsquo;t utilize any RAM at all. Therefore, even if you possess a powerful GPU capable of efficient processing, it seems unlikely that it would greatly enhance the performance of this particular version of the llama program. Nevertheless, it&rsquo;s still fascinating to observe!</p><h2 id=conclusion>Conclusion</h2><p>What have we learned from this analysis? Optimizations play a vital role in programming, the choice of programming language can significantly impact performance, and Python can be complex to configure for low-level operations. Due to these reasons, I refrained from making a direct comparison in this regard.</p></section><footer class=article-footer><hr style=width:100%><section class=article-tags><a href=/blog/hugo/tags/artificial-intelligence/>artificial-intelligence</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/hugo/posts/2023/december/process_simplify_modules/><div class=article-details><h2 class=article-title>Simplifying frontend frameworks</h2></div></a></article><article><a href=/blog/hugo/posts/2023/october/dependency-on-dependencies/><div class=article-details><h2 class=article-title>The doubled edge sword of modules when developing</h2></div></a></article><article><a href=/blog/hugo/posts/2023/april/bash-vs-powershell/><div class=article-details><h2 class=article-title>Bash Vs Powershell</h2></div></a></article><article><a href=/blog/hugo/posts/2023/may/may-the-4th/><div class=article-details><h2 class=article-title>May the 4th be with you</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 Graham's Blog</section><section class=powerby>Graham Steffaniak<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></div></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/hugo/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>