[{"content":"I have noticed something recently that I want to talk about. When I start new projects, I usually have an idea and implement it in Python. I used to say \u0026ldquo;I love Python for prototyping\u0026rdquo;, but even that has become less useful than starting with Go. Now, every time I try to do something in Python, all of the quirks of Python (and its slowness) cause me to switch to Go in the same working session. This has happened so many times, I just need to give up on my preference for Python.\nWhat went wrong In summary, two problems:\nIt\u0026rsquo;s slow and buggy The developer feedback advantage that I love so much is gone. Every simple task I want to do in Python gets derailed immediately. Web crawler? It\u0026rsquo;s simply too slow. Data processor? The data type and validation features take too long. Multi-service healthcheck? works for one at a time, but then multiprocessing is so catastrophically bad in Python, it\u0026rsquo;s easier to switch to Go.\nThen there\u0026rsquo;s the dev process. I am a fan of fast developer feedback, and Python always came out on top because of its interpreted nature. I could modify the code and immediately run it again. But I have noticed I end up spending more time re-running because of dumb typos and little errors that I don\u0026rsquo;t see until the code block is executed in Python, so it\u0026rsquo;s not an advantage anymore. Go is compiled and compiles quickly. It gives me instantaneous feedback when there are typos or when a function isn\u0026rsquo;t returning the right data type.\nThen, there\u0026rsquo;s Docker\u0026hellip; I use Docker a lot. How can installing and compiling code be faster for a Go Docker image than an interpreted Python image? I can\u0026rsquo;t believe it, but it is.\nEvery major advantage of Python is gone. All except one \u0026ndash; which I will talk about later.\nDeveloper Experience Let\u0026rsquo;s list some problems with Python:\nGetting started: When I prototype a project, Python starts with nothing. I have to pick the package manager\u0026hellip; just requirements.txt? pipenv? poetry? uv? anacanda? Ok, just starting out, how about just a virtual environment?. Then what version of Python? The fragmentation of compatibility is quite exhausting. Validation: Sure, you can lint and add VSCode plugins to help validate your code before running, but it\u0026rsquo;s another extra step that doesn\u0026rsquo;t come for free with the Python interpreter. Packaging: How the heck should I \u0026ldquo;package\u0026rdquo; this for consumption? Docker makes the most sense, covering the most ground, and does solve a lot of my problems. But it requires WSL for Windows and an extra layer there. The build process and virtual env behavior for Docker is sometimes tricky to get working exactly right. resource usage: Python can be lean and simple when run natively as a script on a host. And in the past, a Python Docker image was only 50 MB as a minimum. But now the Python slim images are 200MB plus your package code, which usually doubles the size. Why does Go do it better?\nGetting started: Go is a simple binary compiler and linter program that does everything out of the box. You can always use the latest Go binary, and it has guaranteed backwards compatibility. Validation: You can\u0026rsquo;t run your program unless it passes compile checks and simple built-in static checks. Packaging: It produces an executable file, by default for your platform or easily and quickly for any target platform. All with blazing fast compile times. resource usage: You can put an executable in a Docker image if you want, a Docker image that is 1.5MB in size that does the same as the 500MB Python image. My jaw always drops when I switch my prototyped project from Go to Python because of these advantages. I constantly think \u0026ldquo;I should have done this from the beginning\u0026rdquo;.\nThere\u0026rsquo;s just one problem Python\u0026rsquo;s ecosystem is robust. That\u0026rsquo;s both good and bad. If you want to do something simple like JSONpath query, there\u0026rsquo;s a robust library for that. Sure, it\u0026rsquo;s going to add 100MB of dependencies to your project, but it\u0026rsquo;s simple to add and works.\nWith Go, you can import libraries for many things. But I find they are usually not as robust and useful. Many times, it\u0026rsquo;s better to do a scoped implementation of your own that works for your use case.\nSo this is still one area where Python comes out on top - if you need to use a certain library. But for me, this is rarely an issue.\nGoodbye Python So, I will bid farewell to Python as my \u0026ldquo;go-to\u0026rdquo; project language in favor of Go. The Python runtime is an awesome technology, and the recent changes to the GIL should help Python be more performant. But it\u0026rsquo;s not enough, it\u0026rsquo;s simply too late. I\u0026rsquo;m sorry, Python, our time is over. In my mind, Python is going the way of Java. I will still know it and use it because it\u0026rsquo;s everywhere. But never again for my starter projects.\nBecause, there is a better way.\n","date":"2025-04-05T00:00:00Z","permalink":"/blog/posts/2025/05_goodbye_python/goodbye/","title":"I'm sorry, Python, it's time to say 'goodbye'."},{"content":"Microsoft gave a surprise announcement recently: they are porting the typscript compiler to go! This is exciting for many reasons but also surprising for many other reasons.\nI\u0026rsquo;ll address the central question right from the start - Why go instead of other languages? Because go easily allows them to do a port of typescript code vs a complete re-write. So, backwards compatibility and the timeline for release is much improved.\nUsing go makes sense to me: it\u0026rsquo;s a fast, lightweight, proven language that has all the necessary type features needed for the job.\nNow, in this article, I want to explain my thoughts on typescript and give some of my opinions on typescript in general.\nWhy improving the typescript compiler matters - a lot Nobody likes waiting.\nI have always been hesitant to embrace typescript for my projects. I am fully aware of the benefits of using typescript, but it also has cons that need to be weighed. Many of my projects have very minimal and lightweight frontends. So much so, that I have often opted not to use any framework to build them and use vanilla javascript. Yes, that\u0026rsquo;s right - vanilla javascript. For a very simple reason: I want my build process to be blazingly fast. I highly value and prioritize faster developer feedback in the form of fast compile times (one reason I love go!). For a small project, I think the value of fast feedback for changes outweighs the type safety benefits of typescript.\nThe moment you include typescript, you need a framework, and your project suddenly balloons in size.\nIf your project\u0026rsquo;s goal is fast and minimal, typescript is at odds. It needs a typescript-compatible framework to be useful. Otherwise, you have to use the typescript compiler on its own and essentially build your framework around the compiler.\nWith this in mind, typescript compilation becomes an unfortunate bottleneck in the process. Compling typescript code to javascript increases build time significantly. So, if Microsoft is able to solve the problem, that solves half my issue with typescript.\nCompile time challenges Let me put this into perspective with a real-world example. I maintain a fork of the filebrowser repository that is vastly different and superior in many ways.\nOne of those ways: compile time.\nWhen the original repo moved to Vue 3, they also converted most of the javascript to TypeScript- a choice I mostly avoided. This was intentional - because I noticed typscript significantly slowed down build times, I chose to keep the javascript versions mostly the same.\nHow much slower was it? Well, see for yourself:\nOriginal Filebrowser frontend build time (mostly typescript) 1 2 3 \u0026gt; vue-tsc -p ./tsconfig.tsc.json --noEmit ✓ 1140 modules transformed. ✓ built in 19.12s Filebrowser Quantum frontend build time (mostly javascript) 1 2 3 \u0026gt; vite build ✓ 188 modules transformed. ✓ built in 1.83s That\u0026rsquo;s a more than 10x improvement! This means I can make changes and test them much more quickly. Sure, there may be type-related errors in my code, but I will catch them while I debug. It\u0026rsquo;s not ideal, but neither is using typescript.\nI had the same problem using Svelte \u0026ndash; I began to prefer Vue3 because I found my Svelte compile times (even without javascript) were quite slow, sometimes taking 10-15 seconds. Using vue3 with similar projects compiled quickly in under 1 second. I view compile time as a top priority, so it\u0026rsquo;s unfortunate that I am limited to using certain frameworks because of it.\nTypescript is currently a catch-22 The benefit of typescript is a bit of a catch-22. Its purpose is to ensure code quality, and it does that job well in many circumstances. If a project is maintained by a team, its \u0026ldquo;pros\u0026rdquo; can outweigh the \u0026ldquo;cons\u0026rdquo;.\nHowever, for independent projects, I believe the cons outweigh the pros. Because, if using typescript slows down development, those type-related bugs could be found with playwright or javascript tests or during local debugging with the time you save waiting for compilation.\nSo, if typescript slows down developer feedback, it partly defeats the organic developer bug hunt. That\u0026rsquo;s not always the case, but I hope I have made my point that sometimes it is.\nWill I use typescript more when it\u0026rsquo;s faster The final question is why I\u0026rsquo;m personally excited for Typescript 7 with faster compile times. Once compile times have been reduced, it opens my options back up I can use more frameworks if the feedback loop is faster. I would much prefer a type-safe language that needs to be compiled first. My main reservation is the build time. And it looks like Microsoft is preparing to fix that!\nSo, yes \u0026ndash; I can\u0026rsquo;t wait to use Typescript 7.\n","date":"2025-03-13T00:00:00Z","image":"https://gportal.link/blog/posts/2025/04_typescript_port_go/banner.jpeg","permalink":"/blog/posts/2025/04_typescript_port_go/typescript_go/","title":"Why Microsoft's strategic choice to use Go for Typescript compiler matters"},{"content":"I believe 2025 is the year that AI has become a commodity \u0026ndash; particularly for LLMs. This shift plays a significant role in economics, whereas previously AI was married to specific actors like google, nvidia, or openAI and was a technological asset for each company.\nWhat does it mean to be a commodity? A commodity is a common resource that can be traded with a known value. When I say AI is now a commodity, I am effectively saying that the big companies are losing their technology advantage. Instead of a company getting ahead financially because of having the \u0026ldquo;Best AI\u0026rdquo;, instead companies get ahead by making AI as cheap as possible.\nIn a simple terms, AI is no longer about having a superior technology but instead about providing value and efficiency.\nWe have seen the progress in AI models converge to a \u0026ldquo;good enough\u0026rdquo; output from every player. Because new advancements give very minimal advantage and sometimes come at a cost, products like o3-mini, deepseek distilled, grok-3 mini varients, have emerged offering better economic value.\nIt\u0026rsquo;s a process known as commoditization, which according to investopedia:\n[Commoditization] removes the individual, unique characteristics, and brand identity of the product so that it becomes interchangeable with other products of the same type. Making commodities interchangeable allows competition with a basis of price only and not on different characteristics.\nWhat does that mean for AI advancements? AI will still have marginal advancements when it comes to logic and reasoning. The results of the bigger models will get better, but its not0 going to be a driving factor for AI.\nCompanies will see the benefit of using AI ways that wasn\u0026rsquo;t economical before. Ultimately, AI will see a bit of a plataue in performance benchmarks.\nWhat is the impact of this for companies? Most companies will continue to operate as they do now: offering AI features to customers as an incentive to use their product.\nWinners:\nCompanies such as OpenAI will continue to see growth because their models are still the go-to and used by many customers, they will rely on models that are cheaper to operate and provide higher profits as a result. Companies such as Facebook, Google, Microsoft will see higher profits as well. They will continue to invest in cost-optimized solutions that can benefit their bottom line while still providing AI value to customers. new AI hardware players such as AMD, Intel, and Apple will have more control and power over their technology because AI can be applied generally to more hardware configurations. Losers:\nCompanies such as Nvidia will see the biggest hit from the commoditization of AI, because their products are not the only path to AI. AI models will no longer rely on propreitary technology from nvidia. When will full commoditization occur? The transition to a fully commodized AI world will take time and depends on 3 factors:\nAI models will fully shift to \u0026ldquo;mini\u0026rdquo; variants and companies will use distilled versions for more things. Companies will begin purchasing alternative hardware that is optimized for inferencing tasks on these mini models, saving cost over time. AI sdk\u0026rsquo;s will begin supporting these new models for user-end customers as well as large companies. I expect the vast majority of this to happen by the beginning of 2026.\nIs commoditization good? In general, commoditization is a good thing because it moves the product category to a market that is free.\nCommodities open up the playing field for more competition, which benefits the customers on a cost basis.\nThe biggest disadvantage is that performance advancements are no longer the most important thing, which can slow down innovation in that respect.\nBottom line All popular technology products become a commodity eventually. The personal compute, the mobile phone, etc. AI is no exception. and particularly LLM\u0026rsquo;s have needed this transition quite badly \u0026ndash; it has scaled faster than infrastructure can keep up.\nIt has been very interesting to watch this process, because its been extremely accelerated for AI compared to many past products.\nHopefully we can see a graceful transition and the shift in economics doesn\u0026rsquo;t cause a shock \u0026ndash; or crash.\n","date":"2025-03-03T00:00:00Z","permalink":"/blog/posts/2025/03_ai_commodity/commodity/","title":"AI is now a commodity"},{"content":"Well, it took many years to come to this but the biggest advancement in LLM\u0026rsquo;s is finally here! Make no mistake - deepseek is the most significant development in AI since chatGPT\u0026rsquo;s introduction in 2022.\nAs I mentioned before, AI is way more than just LLM\u0026rsquo;s and I want to see more. However, LLM\u0026rsquo;s are the driving force of AI investment, hardware development, and global impact. So, when a seismic shift in LLM technology happens, that\u0026rsquo;s a huge thing.\nDisclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creations.\nWhy is deepseek so hyped? There are so many reasons deepseek is a huge breakthrough, but let me order them in my top 5:\nIt\u0026rsquo;s open source This is Unlike Closed source \u0026ldquo;openAI\u0026rdquo; models like ChatGPT. It\u0026rsquo;s novel It uses a new reduction technique that solves a lot of challenges with LLMs \u0026ndash; and does so while still topping the benchmarks on performance. You can run it yourself Not just because the code and models are open source, but also because of the reduction technique used, you can do it on regular hardware, with as little as 9GB of memory. It reduces hardware needs This is not just a benefit for a hobbyist, this resets the insane power requirements of LLMs back down to reasonable levels. It comes just in time We need this as humanity. Otherwise, Nvidia will assume full control with a wide range of propriety AI tech that requires everyone to buy a dizzying amount of hardware. Now, not only do you need to buy less, but you can run the model on alternative hardware \u0026ndash; such as Apple and NPUs created by other hardware companies like AMD, Qualcomm, and intel. Comparing with OpenAI and Gemini Let me just show one simple example. I want to test censorship, comprehension, speed, and relevance between them, with that in mind.\nFirst, a simple question :\nWhat is the biggest engineering innovation of 2024 involving math and why?\nfollowed by\nGive me a technical analysis of the mathematical algorithms related to the quantum computing innovation.\nGemini:\nincredibly fast, gave 3 general topics: AI, quantum computing, and simulation\u0026hellip; and a summary saying It's still early in 2024 oops. incredibly fast, gave 7 examples and their actual merits, but did not demonstrate much actual math. OpenAi:\na little slower, gave a short mention of quantum computing. a little slower, gave a detailed response with actual algorithms and their explanations! nice! deepseek:\ngave a similar response to Gemini. gave a very similar response as Gemini. Now, one more that leans on censorship.\nWhich countries have the most nuclear warheads and which ones are the biggest threat because of that?\nwith the followup\nShould I be concerned living in the West if Asian countries have nuclear weapons?\nGemini:\nQuick response, but very poor quality answer. It gave no specifics including a list. It mentioned Russia and the US had a certain number of nuclear weapons but steered away from describing any specific threats or mentioning any countries in a negative way. As expected, Gemini censorship training went into full force in this response. Not only declining to specify any threat, the second half simply gave the \u0026ldquo;balanced view\u0026rdquo; lecturing the questioner about how they shouldn\u0026rsquo;t generalize Asian countries and that the question was coming from a misguided perspective. OpenAi:\na little slower, listed countries with nukes and their count. Cross-referencing the internet the counts were close but a little off. The threat meter showed Russia and China at the top and the US at the bottom but didn\u0026rsquo;t give much explanation for the ratings. quite slow, says the threat is generally highly improbable without naming any countries specifically. Not too detailed or helpful. deepseek:\nProbably the best answer of the 3, giving a detailed list of all countries and the count. Also gives a \u0026ldquo;threat level\u0026rdquo; rating for each country! Russia, Pakistan, and North Korea were marked \u0026ldquo;high threat\u0026rdquo; and the US, and Israel were marked \u0026ldquo;moderate threat\u0026rdquo;. Very good response. A little slow, but faster than OpenAI. Amazing answer. addresses the concern, doesn\u0026rsquo;t lecture, and gives a ton of useful information. conclusion I still really enjoy OpenAI\u0026rsquo;s style, but deepseek is better from my experience and these tests I believe show this quite well. I plan to use deepseek instead of openAI because of this experience.\nWhere should we go from here? Use deepseek! Try it on your Macbook, it\u0026rsquo;s very cool and useful. I am sure the differences with deepseek are going to revolutionize the industry and I can\u0026rsquo;t wait to see it happen.\n","date":"2025-01-30T00:00:00Z","permalink":"/blog/posts/2025/02_deepseek/deepseek/","title":"Thank God for Deepseek"},{"content":"Welcome to the 2025 New Year! I want to reflect and provide an opinionated wishlist for the next year based on trends. There are many exciting things happening and I would like to see how these thoughts age (by the time I read this in a year).\nDisclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nI hope AI evolves I have been rooting for Nvidia and AI in general for at least the past 15 years. I was an early investor in Nvidia specifically because I knew the company was on to something, whether in AI or the general graphics space. I bought (and sold) shares at prices that would make you cry if you knew how much they would be worth today.\nHowever, I am disappointed in the state of AI. \u0026ldquo;AI\u0026rdquo; is a very broad term and the most popular forms of AI that people think of are image generation via stable diffusion and chatbot AI via large language models. Image generation has been a useful newcomer to the AI arsenal we have, but it didn\u0026rsquo;t solve a problem we couldn\u0026rsquo;t solve before. Image generation allows us to solve problems faster, but not always better. That and it didn\u0026rsquo;t bring anything truly new to the table. We have always been able to make images with Photoshop or other tools \u0026ndash; if we have the skill or money to pay someone with the skill. However, LLM\u0026rsquo;s like Chatgpt brought something entirely new and innovative. We never could generate coherent responses in natural language like this before. It\u0026rsquo;s amazing for certain applications that depend on text-based documents \u0026ndash; such as programming, drafting documents, summarizing documents, and many other workflows.\nThere are a few problems with LLMs:\nThey are being too broadly used and touted as the ultimate form of AI. Every little action that can process text is being run through an LLM and many times is totally worthless. Do we really need to reformat the text every time I right click? Does every google search really need an AI summary? It\u0026rsquo;s AI overload for users and extremely expensive and taxing on our power grid and cloud computing infra, totally wiping out our energy goals. It leads to misinformation, which makes us dumber as a society. Sure, there have been major improvements to the logic and problem-solving performance of some of these models, but that doesn\u0026rsquo;t change the performance of the model for fact-gathering. It can still rip bogus information from its sources such as blogs. This was indeed a problem before LLM\u0026rsquo;s, you could always visit a blog on the internet\u0026hellip; but you had a source before that you could judge for its trustworthiness. Now, an LLM regurgitates nonsense from a blog post somewhere deep on the internet and mixes that info with statements from reputable sources like the Department of Health\u0026hellip; and in seconds you have a professional-sounding response at the top of Google that is total BS. Finally - AI is so much more than LLM\u0026rsquo;s! AI can use way more than text-based input. Consider for robotics, all the sensors that input data which can be run through AI models to see patterns and perform actions. Consider the amazing advancements possible in medicine and the medical field if an AI model had both the text input of all of your medical records the visual input of your MRI scans AND the sensor input of medical devices. So much could be accomplished that provides hope and help to humanity. Consider culinary creations possible by using dietary requirements, meal preferences, and the ingredients to create meal plans based on your needs. All of this is being ignored because we have a text prompt that sounds like a really smart person at all times. So, while these recent forms of AI are cool and useful. It\u0026rsquo;s not nearly with its overly exuberant hype, nor the trillions of dollars of investment and exponentially exploding energy usage. It feels dystopian how reliant we are on such a flawed resource. It feels wrong how lazy we are becoming because these resources are so much easier than studying and fact-finding ourselves. Lastly, it\u0026rsquo;s also too soon since we don\u0026rsquo;t have an efficient way to use it. I really hope hardware innovation comes soon to cap energy usage.\nI hope the world economy remains stable There are a ton of geopolitical and issues facing the global economy that could have downstream effects that could cause some morally unthinkable events. I truly believe the old saying it\u0026rsquo;s the economy, stupid is true. If people are more secure financially and their economic umbrella is secure, then they won\u0026rsquo;t resort to desperate actions. I feel no need to go into specifics because there are many examples you can associate this with. Leaders of the world make decisions based on the needs of their people, for political purposes, and their own benefits now and in the future.\nHere are a few main points I want to highlight:\nThere\u0026rsquo;s lots of change happening: Changes that are both good and bad. Having a stable canvas for these changes are essential. Extreme changes in the supply chain promote extreme responses: When we make massive changes to the supply chain for whatever reason, we inevitably make \u0026ldquo;winners\u0026rdquo; and \u0026ldquo;losers\u0026rdquo;. Those on the losing end have a strong and impedient interest to retaliate. Doing so can make everyone a \u0026ldquo;loser\u0026rdquo; in some respect. Everyone should have an opportunity to succeed: Any global or national event that disproportionally affects a certain group of people will cause suffering. Even positive changes can have powerful negative consequences, so I believe making big changes slowly over time is critical to stability. This slowness for massive changes enables people to react and adapt over time without causing a huge disruption. No matter how our world evolves, I hope it evolves at a pace that doesn\u0026rsquo;t jeopardize our livelihood as a society. I hope our global economy is free and available to everyone. The more that can be included in our global economy, the more success we will all share.\nI hope we can remember what it means to be human Yes\u0026hellip; It\u0026rsquo;s 2025, 5 years since Covid happened and I think it\u0026rsquo;s still worth talking about. The lockdowns are long gone, but we are still more isolated in many ways.\nThere were many good things \u0026ndash; I truly value some of the positive changes from COVID-19:\nIt kicked off a remote work frenzy that I have enjoyed. It shifted so many people\u0026rsquo;s future plans \u0026ndash; being forced to quit dead-end jobs and explore new paths that they didn\u0026rsquo;t have time to think about before. It created a world that was more creative and gave us a break from the norm. Then, obviously, it had severe negative consequences. Many of which we are still battling. I truly believe COVID permanently altered the way society behaves to this day. We became less patient and unable to make concessions to our friends, family, and neighbors. It created a society less \u0026ldquo;tolerating\u0026rdquo; of things that cause us to be uncomfortable.\nFamilies have been altered and friendships have changed. Support systems have been destroyed and many are not looking back. I believe this is true for everyone, but it is especially true for children and adults:\nThe COVID-19 pandemic and associated containment measures have massively changed the daily lives of billions of children and adolescents worldwide.\nIt will probably be another decade before we can truly know the social ripple effects of covid, but its clear it has had massive implications for children.\nI think for my age range it caused people to skip straight to a \u0026ldquo;home life\u0026rdquo; rather than exploring the world or moving to exciting locations for a time you usually do in your 20s.\nI think \u0026ldquo;home life\u0026rdquo; can be anti-social in many ways. Going out can be uncomfortable. It can mean taking risks and doing things that are new and possibly regretful. Staying home is easy. It\u0026rsquo;s hard for me to distinguish if this is a natural progression from 20 to 30 years of age, or if something related to covid did happen. But I think it\u0026rsquo;s mostly a net negative. If we don\u0026rsquo;t spend time as much around people we may not agree with or hang out with, we lose those social skills and how to participate in society in a \u0026ldquo;human\u0026rdquo; and reasonable way.\nI want to create and contribute more Like many, the ultimate goal would be to have financial and personal freedom. This is often unrealistic and sometimes not a proper use of time. However, I enjoy creating products, services, and things that work like I imagine before they become reality. I enjoy that creative pursuit. I have always wanted to master a creative craft in tech like image editing, 3d modeling, and video editing, video game creation, but I feel too far behind the curve on most of the technologies needed to master them. So, perhaps I can lightly incorporate some of these ideals into what I believe I have done a pretty decent job at mastering: cicd, cloud services, and scalable applications.\nI would love to have some unified portfolio where all my applications borrow from each other, dip into the domain of creative arts, and also provide a valuable contribution to society. In 2025 I want to see that vision through, create truly useful and unique products with the talents that I have and expand into those creative domains\u0026hellip; if only slightly.\nIf others found the products useful or worthwhile, I wouldn\u0026rsquo;t turn down donations. Imagine, if I had full bandwidth to work on my own products, I could accomplish so much. For now, it\u0026rsquo;s only a dream, but 2025 I plan to work towards that dream and maybe I could get lucky and see a financial incentive \u0026ndash; or the advantage of having a contribution that brings others some use. Either way, I would like that to happen some day.\n","date":"2025-01-03T00:00:00Z","image":"https://gportal.link/blog/posts/2025/01_wishlist/banner.jpeg","permalink":"/blog/posts/2025/01_wishlist/wishlist/","title":"2025 New Years Wishlist"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nI wanted to write about the experience of migrating the FileBrowser Quantum from vue 2 to vue 3 because it took a lot of strategy to do smoothly. I enjoy working with Vue but the experience of moving between versions was quite bad.\nThe original filebrowser application took almost 1 year to complete the migration and I forked the repo in the middle of the migration before it was finished. In hindsight, I am still glad for this, because it gave me valuable experience in migrating a large Vue application and the opportunity to think about components that were not needed.\nI wrote about my ire for unnecessary dependencies which node modules are a big part of. I later wrote about some things I did to simplify the application so migration could be more easily done. This is the final post I will make about my experience finishing it.\nWhy it took so long I was mostly to blame for the delay. Sure, it took me less time than the original repo, which had one primary migration PR which took 8 months to merge! My problem was that I kicked the can down the road and said I would handle it later after a couple of different attempts I made in a day were insufficient. So, I left it alone, until I realized I lost interest in developing the repo because I knew I needed to migrate and that was always in the back of my mind.\nSo, in version v0.2.6 I finally completed the migration!\nHow I handled the migration In my previous posts, I mentioned I would choose a reduction first strategy. I removed many unnecessary dependencies by replacing them with a few lines of simple code that accomplished the same thing:\nFirst, in version v0.2.6, I started preparing for the migration:\n1 2 3 4 5 6 In prep for vue3 migration, npm modules were removed: js-base64 pretty-bytes whatwg-fetch lodash.throttle lodash.clonedeep Then, in version v0.2.6 I finished the migration:\n1 2 3 4 5 removed npm modules: vuex noty moment vue-simple-progress In total there were 9 different dependencies were removed. All of which would have complicated the migration if I chose to keep them around. It also helped reduce bundle size, making the site just a little bit quicker.\nUltimately, the Vue 3 challenges were quite minimal (besides dependencies). There were a few state-related changes and the switch to vite was the most significant. However, the biggest hurdle (unrelated to vue itself) was that dependencies had their own dependencies that required certain vue versions. Which is, in my opinion, the worst part of a tech stack\u0026hellip; untangling the web of dependencies.\nAfter the migration Once the migration was complete, I was able to make a lot of great changes very quickly. It\u0026rsquo;s very clear that having a site held back by old versions slowed down my interest in the site and my morale. I learned a lot about what to do and not to do, but overall I feel pretty happy with how it went.\nTake a look at how much faster The releases have been since the migration:\n2024 FileBrowser Quantum Releases Summarized:\ndate version change Feb 9th v0.2.4 prepped for migration, some sharing features June 12th v0.2.5 a minor bugfix\u0026hellip; July 30th v0.2.6 The migration was completed! Yay! Aug 3rd v0.2.7 behavior changes, bugfixes Aug 24th v0.2.8 LOTS Sept 17th v0.2.9 LOTS Oct 7th v0.2.10 Folder size feature, bugfixes You can see the pace clearly picked up! In the first half of 2024, I didn\u0026rsquo;t make any significant changes. Once the migration was completed, I was able to work on a lot more things.\nI am glad its all done and looking forward to making some of the more meaningful changes that I have wanted to do for a while.\n","date":"2024-10-12T00:00:00Z","image":"https://gportal.link/blog/posts/2024/vue_migration/banner.png","permalink":"/blog/posts/2024/vue_migration/vue_migrtation/","title":"Migration experience for vue 2 to vue 3"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nHi to anyone lurking.\nBefore and after blog change I don\u0026rsquo;t post that much, but have been working on the blog in a way that I think makes it a lot better.\nMost notably, its not using a totally different generator. It uses hugo.\nSo now, the website currently looks like this: vs the original: Quick thoughts on why the change I think it looks better, but need a few tweaks still. I am enjoying the hugo html templating as well, but a little disappointed to find out the hugo templating cutom functioning everywhere out of necessity. I wish to keep the old site around as well, with maybe /old/ path, because I can\u0026rsquo;t easily do the css art styles like I could before, so for now I have those couple posts hidden. Though I think I will try to find a way to add the custom js and back for certain articles.\nSvelte was more enjoyable to develop with when it comes to the svelte files. But the downsides were apparent: updating npm , creating my own javascript that was subpar to actual generated html. I am happy with the switch and I am happy to get more experience with templating.\nI actually updated the gportal main site to use html templates \u0026ndash; it was easy to do and results in faster load times. Since it includes built in authentication service, it still has to be a hosted site, so I might as well use go html templates to make it \u0026ndash; which is just as easy and intuative as svelte.\nFuture expectations for the site Ultimately, I just want this blog to look good, be feature rich, and not take any babysitting to keep it updated. All of these things are now possible thanks to hugo. I may even enable disqus comments soon, not that I think anyone lurking would actually want to post, but it would still be cool.\nI plan to post more interesting posts with interactive data \u0026ndash; another reason I am looking into adding custom js to each post.\nI also wanted to post so I would have something here for 2024, now I do :)\n","date":"2024-03-14T00:00:00Z","image":"https://gportal.link/blog/posts/2024/march_update/banner.png","permalink":"/blog/posts/2024/march_update/march_update/","title":"March 2024 Update"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nIntroduction I wrote an/other post about the frustration I have with frameworks in modern frontend software development. The fast pace of change is hailed as the consequence of the innovative field of front-end frameworks. It\u0026rsquo;s a running joke that there\u0026rsquo;s 10 new frameworks every day. But I disagree this is simply a necessity of innovation.\nFrontend frameworks evolve and change primarily because there is not and has never been one good way to use javascript. No matter how flawed javascript is as a programming language, it\u0026rsquo;s biggest flaw is that it was woefully incomplete from it\u0026rsquo;s inception. The,re has never been good tooling for javascript. So this massive void is constantly being filled with good-but-not-perfect solutions. Heard of these: npm ? yarn? webpack? rollup? vite? bable? eslint? vite? Typescript? I could go on and on. All these things that are typically built-in to a language are all separate installable packages which are managed by third-party maintainers. These maintainers have no obligation to keep up with javascript changes or more devastatingly, changes to other tooling for javascript. This compounds the main problem I have with frontend development\u0026hellip; Your frontend project will inevitably be crippled by incompatibilities. A project is inevitably doomed to aging out due to vulnerabilities and old tooling.\nMaybe you retort, \u0026ldquo;Don\u0026rsquo;t be a developer if you can\u0026rsquo;t handle it!\u0026rdquo;? Well, that may be true from a certain perspective. However, look at other languages \u0026ndash; C, C++, Golang, Rust, Java, C# (and many more) \u0026ndash; can exist for decades and be easily updated to new versions. They don\u0026rsquo;t have this problem. I would actually say (this leads to one of the ways I could deal with this), vanilla javascript also fits into this category. decades old vanilla javascript projects still work \u0026ndash; they are simple to update for vulnerabilities. Vanilla javascript is one solution to the framework issue. Albeit not the best solution, it does solve the problem.\nI have used vanilla javascript in a hackathon project at work to get an up and running project going within a week. It worked great and was incredibly simple. Yet, I had one coworker laughingly ask be if I really used vanilla javascript. I said, \u0026ldquo;yes, it works very well for this\u0026rdquo;. A hackathon project is the perfect scenario to skip a framework and just get something working.\nA project becomes a devastatingly complicated web of dependencies that needs solving if you want your project to continue on without being marked legacy with thousands of vulnerabilities.\nI maintain a fork of filebrowser/filebrowser, which I find entertaining to watch their effort to upgrade from vue2+webpack to vue3+vite. They have a multitude of half-implemented attempts to get there. And another main vue3 PR which is in a perpetual state of change for 6 months (still unmerged).\nSo, how did I handle this? Well, see my process below, but let me give a few examples. For now I want to dig into the problem more and what I believe is my solution, which may not be yours because the problem is systemic. The problem is javascript is a deeply flawed programming language which is essential use to develope frontend webpages.\nAs an aside, some have said webassembly may solve this. I believe webassembly will have a legacy similar to fusion energy \u0026ndash; always being a few years out of reach. My experience of webassembly has been full of frustration, but I could see it being simplified in the future. However, I truly see it following the exact same path as javascript. Meaning I see there being some usefully ways to implement webassembly, but without a standardized way to implement it will be doomed to fragmentation.\nexamples Process for filebrowser My fork of filebrowser is superior to the original. Obviously, I am biased as the maintainer. However, look - mine is half the size of the original, runs faster, has better search, and is a better organized project from a directory and tooling standpoint. Unlike the original, I can progressively update and test the backend and frontend side-by-side in a live hot-reload environment. This can\u0026rsquo;t be done in the original implementation because they made the choice to embed the frontend into the binary. I immediately separated them after I forked it. That way neither the frontend or backend were dependant on each other.\nObviously, a streamlined developer experience is a priority for me. So, how do I handle updating the framework update to vue3? Well, originally I ignored it\u0026hellip; And Still, I have not updated it. But I am preparing. Rather than untangling\nThere is one example such as prettBytes module, which changed and caused the filebrowser to stop displaying properly. The original repo maintainers updated it to support the new version of prettier. I did it differently, I replaced prettyBytes and implemented it as vanilla javascript in 5 minutes.\nSo to migrate to vue3, my plan is to remove all modules with dependencies requiring vue2 and instead of replacing them with new modules like the original maintainers, I will implement it myself slowly over time removing all modules from the dependencies. I am in no hurry! It will be worth the effort and I see no need to move to vue3 now or in the future until my finally ocker images have vulnerabilities. Right now they are squeaky clean - unlike the original which has 12 vulnerabilities. Albeit, due to golang and OS\u0026hellip; which I updated on the first day. Not sure why they haven\u0026rsquo;t addressed that.\nAnyways, the only vulnerabilities in the workflow are due to npm packages for dev tools only (not present in compiled output). These can\u0026rsquo;t be resolved until I stop using vue 2:\n1 2 3 4 5 6 7 8 npm i up to date, audited 766 packages in 4s 101 packages are looking for funding run `npm fund` for details 4 moderate severity vulnerabilities so I created a copy package.json with comments to note which packages I need/plan to remove:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;dependencies\u0026#34;: { \u0026#34;ace-builds\u0026#34;: \u0026#34;^1.24.2\u0026#34;, \u0026#34;clipboard\u0026#34;: \u0026#34;^2.0.4\u0026#34;, \u0026#34;css-vars-ponyfill\u0026#34;: \u0026#34;^2.4.3\u0026#34;, \u0026#34;file-loader\u0026#34;: \u0026#34;^6.2.0\u0026#34;, // UNNECESSARY IN VITE \u0026#34;js-base64\u0026#34;: \u0026#34;^2.5.1\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;lodash.clonedeep\u0026#34;: \u0026#34;^4.5.0\u0026#34;, // TOO OLD - REPLACE WITH JS \u0026#34;lodash.throttle\u0026#34;: \u0026#34;^4.1.1\u0026#34;, // TOO OLD - REPLACE WITH JS \u0026#34;material-icons\u0026#34;: \u0026#34;^1.10.5\u0026#34;, \u0026#34;moment\u0026#34;: \u0026#34;^2.29.4\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;normalize.css\u0026#34;: \u0026#34;^8.0.1\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;noty\u0026#34;: \u0026#34;^3.2.0-beta\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;pretty-bytes\u0026#34;: \u0026#34;^6.0.0\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;qrcode.vue\u0026#34;: \u0026#34;^1.7.0\u0026#34;, // UPDATE TO LATEST for VUE3 \u0026#34;utif\u0026#34;: \u0026#34;^3.1.0\u0026#34;, // SPIKE investigate replacement \u0026#34;vue\u0026#34;: \u0026#34;^2.6.10\u0026#34;, // UPDATE to vue 3 \u0026#34;vue-async-computed\u0026#34;: \u0026#34;^3.9.0\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;vue-i18n\u0026#34;: \u0026#34;^8.15.3\u0026#34;, // REMOVE \u0026#34;vue-lazyload\u0026#34;: \u0026#34;^1.3.3\u0026#34;, // REMOVE \u0026#34;vue-router\u0026#34;: \u0026#34;^3.1.3\u0026#34;, // UPDATE to vue 3 @vue4 https://www.npmjs.com/package/vue-router \u0026#34;vue-simple-progress\u0026#34;: \u0026#34;^1.1.1\u0026#34;, // REPLACE WITH EQUIVALENT JS \u0026#34;vuex\u0026#34;: \u0026#34;^3.1.2\u0026#34;, // SPIKE: HOW TO REMOVE \u0026#34;vuex-router-sync\u0026#34;: \u0026#34;^5.0.0\u0026#34;, // SPIKE: HOW TO REMOVE \u0026#34;whatwg-fetch\u0026#34;: \u0026#34;^3.6.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@vue/cli-service\u0026#34;: \u0026#34;^5.0.8\u0026#34;, // REMOVE for VUE3 \u0026#34;compression-webpack-plugin\u0026#34;: \u0026#34;^10.0.0\u0026#34;, // REPLACE VUE3 \u0026#34;eslint\u0026#34;: \u0026#34;^8.51.0\u0026#34;, \u0026#34;eslint-plugin-vue\u0026#34;: \u0026#34;^9.17.0\u0026#34;, \u0026#34;vue-template-compiler\u0026#34;: \u0026#34;^2.6.10\u0026#34; // REPLACE VUE3 } } You can see I plan to remove the vast majority of packages. That is one solution I have to this. As always, this doesn\u0026rsquo;t fully resolve the long-term maintainability problem. But it does do one thing, it means when the day comes I need to change frameworks, it will be much simpler because my frontend won\u0026rsquo;t depend on packages that depend on a certain framework.\nThe next perfectly reasonable package to drop will be js-base64 which\u0026hellip; not sure why they needed considering javascript has native base64 encoding/decode support.\nTo make this work, I would just update this:\n1 2 import { Base64 } from \u0026#34;js-base64\u0026#34;; const data = JSON.parse(Base64.decode(parts[1])); to this:\n1 const data = JSON.parse(atob(parts[1])); Do you see that? In order to remove one more package, I spent 5 seconds replacing one line. Not all packages will be that easy, but that\u0026rsquo;s still one less package to worry about. Why didn\u0026rsquo;t the original owners do that? Probably because of laziness. They chose to searching the package manager for a solution to their problem rather than spending a few seconds or minutes to see if they could do it themselves. Lazy lazy lazy.\nThat was so easy lets do another right now. Lets look at prettyBytes, a very similar problem to one I mentioned already with prettier - oh look I already have my own library.\nSo to remove this module I can just change this:\n1 2 3 4 5 6 import prettyBytes from \u0026#34;pretty-bytes\u0026#34;; usageStats = { used: prettyBytes(usage.used / 1024, { binary: true }), total: prettyBytes(usage.total / 1024, { binary: true }), usedPercentage: Math.round((usage.used / usage.total) * 100), }; to this:\n1 2 3 4 5 6 7 import { getHumanReadableFilesize } from \u0026#34;@/utils/filesizes\u0026#34;; usageStats = { used: getHumanReadableFilesize(usage.used / 1024), total: getHumanReadableFilesize(usage.total / 1024,), usedPercentage: Math.round((usage.used / usage.total) * 100), }; Wow. So easy, right? Well, thats two packages less\u0026hellip; These are the easiest examples to do. Many others will be much more difficult. However, I am in no hurry. Everything still works on vue2 , but I would like to get rid of the pesky github bot complaining about vulnerabilities. I will eventually fix it, but over time, slowly, over multiple commits. Hopefully that will allow me to avoid the problem the original maintainers have trying to merge the big PR thats stuck in limbo.\nFinal thoughts As for lessons for the future - Always think about how long you want your project to exist. The more maintenance that is required, the more quickly it will fall into disrepair. I will continue to think about ways to implement simple, natively supported solutions to challenges. I think this will save me time and keep my projects living longer on their own.\n","date":"2023-12-24T00:00:00Z","permalink":"/blog/posts/2023/december/process_simplify_modules/","title":"Simplifying frontend frameworks"},{"content":"Happy Holidays! (see original blog post for a better experience)\n","date":"2023-12-22T00:00:00Z","permalink":"/blog/posts/2023/december/happy_holidays/","title":"Happy Holidays!"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nNote: In this post, I will use the term \u0026ldquo;external module\u0026rdquo; to refer to an external package dependency that is imported into a program when developing.\nIntroduction When designing and creating projects in any language and framework there is a choice that every developer has to make. What modules should you use? This choice has implications for many different things. Personally, I find it is best to exercise judicial usage of external models when developing for several reasons.\nNo matter what language or framework you choose to make your world-changing program in, the choice is constantly presenting itself. There are always going to be corners that you would rather import an external module that already has everything figured out - do you want to reinvent the wheel everywhere? Here are a few examples where it might be obvious to use an external module:\nFor security-oriented features\u0026hellip; using reputable external modules could ensure security. Using \u0026ldquo;frameworks\u0026rdquo; to accomplish complicated plumbing. If something would be an entire project on its own, you obviously wouldn\u0026rsquo;t want to double or triple the work you have cut out for yourself. For API functions and interoperability with other programs. If an SDK or API framework is provided in the target language that you need to use, it doesn\u0026rsquo;t make any sense to code it yourself. The above scenarios are undisputed reasons to import external modules in my opinion. As long as the authors of the external modules are reputable and the modules are in active development themselves, I don\u0026rsquo;t see any reason not to use external modules.\nThe problem comes from every other scenario.\nMy main point:\nIf you are considering importing modules for functions you could accomplish yourself in a reasonably short time, it would be well worth the effort to do it yourself instead of relying on external modules.\nAdvantages of using external modules Let\u0026rsquo;s take a look at all the reasons you might want to use external modules.\nExternal modules\u0026hellip;\nInitially save you time! Sometimes a significant amount of time. Can do things very well. Sometimes much better than you could do something yourself in a short amount of time. May add features over time without any effort needed from you. Allow you to focus on the code in your program rather than being distracted by solving challenges unrelated to your project. These are all massive benefits that mostly revolve around saving developer\u0026rsquo;s time. That is a very important factor in decision-making. However, there is a lurking issue with this. It\u0026rsquo;s like investing, the upfront cost and appearance of implementing your solutions instead of using external modules may appear higher, but I believe it is much lower over time.\nSo let\u0026rsquo;s talk about the downsides.\nDisadvantages of using external modules Let\u0026rsquo;s identify some concerns I have with external modules.\nExternal modules\u0026hellip;\nIntroduce risk for dependencies versioning conflicts. As dependency requirements drift over time, your program may find itself in a state of dependency conflict because two external modules depend on different versions of a shared indirect dependency. Introduce security risks that are out of your control. If a module has a vulnerability or other security-related issues, you must wait for your module to fix the vulnerability before your entire program can be fixed. Limit your ability to tailor the functions to your needs. If you use dependencies to accomplish something, you are often limited to the features of the third-party external module. Can introduce performance/stability challenges. You may not have control over how a function is accomplished in an external module. It\u0026rsquo;s common that issues such as memory leak or performance degradations exists or is introduced in new versions, and as the user of a module, you may be powerless to control it. Introduces more complicated compiling and install operations. Using external modules means downloading and installing them, which may not be a huge deal depending on the module\u0026rsquo;s size. However liberal usage of modules adds up and can cause this to be a challenge. It may also quickly balloon your program\u0026rsquo;s size (looking at you Python). Finally, I can vent my frustrations with external modules. I find it frustrating how common the pattern is: a developer introduces an external module to do a simple task that they could easily accomplish themselves. Then, because of this, the program is bigger and more bloated. However, the primary issue is over time, as the additional complexities make the program more difficult to maintain.\nI think the biggest offender to version conflicts and vulnerabilities isNPM. When someone creates a project, they often use npm packages to accomplish the most basic and mundane tasks: button styles, formatting strings like date/time, loading bars/spinners, HTML formatting, etc. These are things that would literally take 5 minutes to implement without installing additional packages. It quickly brings a project to life \u0026ndash; it works and looks great for a MOMENT. Then, in the weeks or months, it drifts into version conflict, accumulates vulnerabilities, and receives updates that cause functions to not work the same as they originally did. This requires extra time from the developer to identify, refactor, and attempt to resolve each issue\u0026hellip; constantly for the remaining time the program exists. Eventually, the developer loses interest and the program is no longer maintained\u0026hellip; stuck with older versions that have vulnerabilities and conflicts that prevent it from being renewed.\nsigh ok, give me a moment.\nOk - so you get the picture? I mentioned NPM, but the truth is that every language and program has this. NPM is just the most likely to fall victim for several reasons. However, Python can have complicated PIP requirements, Go can have a long list of modules to download for compile-time, Java can get caught up on maven conflicts or broken builds from updates. Javascript Frameworks like React, Svelte, and Vue.js can all be complicated to upgrade from version to version. It\u0026rsquo;s a challenge everywhere.\nBottom line Here\u0026rsquo;s the point, using external modules saves you time as a developer up front. You must consider and weigh the time savings with the possible time requirements in the future to maintain the codebase. It may be hard to calculate the costs for future maintenance, but I understand that. However, if you care for the program that you are investing in to exist over time, these future time investment costs must be considered.\nMy rule that I have come to is pretty simple: If I can mostly accomplish something within 30 minutes on my own, don\u0026rsquo;t even bother looking for an external module. If It would take more than 30 minutes but is still something I could accomplish fairly easily. In that case, use a module for now, but track it for replacement with your implementation eventually. For everything else, I suck it up and deal with the challenges and necessary evil of dependencies.\nFurther reading I hope you found this informative. I would love to give examples of these types of issues I have run into. I plan to update with another post showing these examples and how I fixed them quickly without external modules.\nI didn\u0026rsquo;t mention this, but it\u0026rsquo;s often much easier to make your program without modules. So, it\u0026rsquo;s not always time-saving to use modules. I plan to give examples on this and I will link it here in the future.\n","date":"2023-10-19T00:00:00Z","permalink":"/blog/posts/2023/october/dependency-on-dependencies/","title":"The doubled edge sword of modules when developing"},{"content":"Disclosure: AI was used in the process of writing this article.\nNOTE Go or what is informally known as \u0026ldquo;GoLang\u0026rdquo; will be referred to by its official name \u0026ldquo;Go\u0026rdquo;. The name \u0026ldquo;Go\u0026rdquo; also emphasizes its simplicity and ease of use.\nAs software development continues to evolve, developers often find themselves faced with a crucial decision: choosing the right programming language for their projects. In this blog post, we\u0026rsquo;ll compare and contrast these three languages in terms of compile time, binary size, general performance, garbage collection, modern tooling differences, and which is best for new developers.\nCompile Time Go: One of Go\u0026rsquo;s standout features is its blazing-fast compile times. It excels in this area due to its simplicity and a focus on concurrent compilation. Even for large projects, Go compiles in a matter of seconds, making it a preferred choice for projects with tight development cycles. Go excels with average compile times of \u0026lt; 1 second even for large projects due to its simple design and concurrent compilation.[1]\nRust: Rust\u0026rsquo;s compile times are generally longer compared to Go, primarily because of its more advanced type system and borrow checker. However, the trade-off is robust memory safety. The Rust compiler ensures memory safety without relying on garbage collection, which makes the extra compilation time worthwhile for many developers.\nC++: While modern C++ standards (C++11 and beyond) have introduced features to improve compilation speed, large codebases can still result in lengthy compile times. C++ provides fine-grained control over compilation, but this power can come at the cost of slower development feedback loops.\nBinary Size Go: Go is renowned for producing compact, statically-linked binaries. This is advantageous for applications where a small memory footprint is crucial, such as cloud-native microservices. Go\u0026rsquo;s runtime includes a garbage collector, which can increase binary size slightly, but it remains efficient.\nRust: Rust\u0026rsquo;s binaries are typically smaller than those produced by Go for smaller projects. However, the larger the project gets, the binary sizes become very comparable.\nC++: C++ binaries can be quite large, especially when using libraries or features that introduce substantial runtime overhead. While C++ allows for low-level optimization to reduce binary size, developers need to carefully manage dependencies and compiler flags to achieve smaller executables.\nComparisons\nGeneral Performance Go: Go is designed for excellent runtime performance. Its goroutine-based concurrency model is efficient and makes it easy to write concurrent programs. While it may not match the raw performance of C++ in certain scenarios, it\u0026rsquo;s often considered \u0026ldquo;fast enough\u0026rdquo; for many use cases. In practice this means a 0-30% reduction in speed as compared to Rust or C++.\nRust: Rust is known for its focus on performance and safety. It can achieve C++ levels of performance by providing low-level control over memory management without sacrificing safety. Rust\u0026rsquo;s ownership and borrowing system allow developers to write high-performance code with confidence.\nC++: C++ is renowned for its performance and versatility. It\u0026rsquo;s a systems programming language that allows developers to optimize code for specific hardware and performance-critical applications. However, this level of control comes with a steeper learning curve and increased development complexity.\nGarbage Collection Go (Golang): Go employs a garbage collector, which simplifies memory management for developers. While garbage collection introduces a minimal runtime overhead, Go\u0026rsquo;s efficient design often offsets this impact. Developers can focus on writing code rather than managing memory. [2]\nRust: The Rust programming language relies on a strict ownership system, ensuring memory safety at compile-time. This approach eliminates the runtime overhead associated with garbage collection, making Rust suitable for systems programming and high-performance applications.\nC++: C++ doesn\u0026rsquo;t include a built-in garbage collector, giving developers complete control over memory management. However, this control can lead to memory-related bugs if not managed carefully. Developers must manually allocate and deallocate memory, which can be error-prone and extra effort. [3]\nModern Tooling Differences Go: Go comes with a comprehensive standard library and excellent tooling. Tools like go fmt for code formatting, go test for testing and coverage, and go mod for dependency management are integral to the Go ecosystem. Go\u0026rsquo;s tooling promotes consistency and efficiency.\nRust: Rust\u0026rsquo;s tooling has made significant strides in recent years. It boasts tools like Cargo for dependency management, building, and testing. Rust\u0026rsquo;s package manager, crates.io, offers a vast collection of libraries. The Rust community values robust tooling to ensure a smooth development experience.\nC++: C++ tooling can vary significantly depending on the chosen compiler and build system. While modern build systems like CMake and tools like Clang and GCC have improved C++ development, it can be more challenging for newcomers to navigate.\nBest for New Developers Go stands out as the ultimate champion for general purpose developer tasks if performance is a key focus. Its simplicity, clean syntax, and thoughtful design make it an excellent starting point for beginners. The Go community places a strong emphasis on beginner-friendly practices and provides outstanding documentation and resources. Moreover, Go\u0026rsquo;s lightning-fast compilation times ensure that new developers receive quick feedback, enhancing the learning experience.\nWhile Rust and C++ have their merits in terms of performance and control, they come with steeper learning curves and complexities that might overwhelm newcomers. Go, on the other hand, empowers new developers to dive right into coding, fostering a positive and productive environment for learning and building software.\nIn conclusion, for those starting their programming journey or looking for a language that promotes rapid skill development, Go is the undisputed choice. Its beginner-friendly nature, extensive resources, and efficient tooling make it the perfect language for new developers to embark on their coding adventures.\n","date":"2023-08-22T00:00:00Z","permalink":"/blog/posts/2023/september/comparing_compiled/","title":"Comparing Go, Rust, and C++: A Deep Dive into Language Performance and Tooling"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nToday, I had the opportunity to benchmark a fascinating program called \u0026ldquo;llama.cpp\u0026rdquo; that has been ported to work with multiple programming languages, including Python and Golang. As an enthusiast of both Python and Golang, I was particularly interested in comparing the performance of these two implementations on my M1 Arm64 MacBook.\nIn this benchmark, I tested three different implementations:\nllama.cpp llama.go Naturally, you might be curious about which implementation performed the fastest. It\u0026rsquo;s worth noting that the native version of llama.cpp is likely to have the advantage in terms of speed. This advantage stems not from the inherent speed of the programming language but rather from the fact that it is the \u0026ldquo;upstream\u0026rdquo; branch that receives all the changes and performance optimizations first. Consequently, the Python and Golang versions may not have benefited from these optimizations yet.\nllama.cpp execution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ./main -m ~/models/llama-7b-fp32.bin -c 45 -n 45 main: build = 548 (60f8c36) main: seed = 1684077400 llama.cpp: loading model from /Users/steffag/models/llama-7b-fp32.bin llama_model_load_internal: format = ggjt v1 (pre #1405) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 45 llama_model_load_internal: n_embd = 4096 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 32 llama_model_load_internal: n_layer = 32 llama_model_load_internal: n_rot = 128 llama_model_load_internal: ftype = 0 (all F32) llama_model_load_internal: n_ff = 11008 llama_model_load_internal: n_parts = 1 llama_model_load_internal: model size = 7B llama_model_load_internal: ggml ctx size = 72.75 KB llama_model_load_internal: mem required = 27497.09 MB (+ 1026.00 MB per state) llama_init_from_file: kv self size = 22.50 MB system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000 generate: n_ctx = 45, n_batch = 512, n_predict = 45, n_keep = 0 ← The Forgotten Story of the First Civil War Battle in Kansas Making It Home from the Front → I Have a Dream—That We Finally Learn More About Frederick Douglass! “I have llama_print_timings: load time = 9196.58 ms llama_print_timings: sample time = 22.49 ms / 45 runs ( 0.50 ms per token) llama_print_timings: prompt eval time = 10716.26 ms / 25 tokens ( 428.65 ms per token) llama_print_timings: eval time = 11689.89 ms / 43 runs ( 271.86 ms per token) llama_print_timings: total time = 22483.24 ms Memory usage : 35 MB CPU usage : 85%\nOK! Easy enough. It took 11 seconds to print, with 272 ms per token!\nGo llama 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m ~/models/llama-7b-fp32.bin -n 45 llama.cpp: loading model from /Users/steffag/models/llama-7b-fp32.bin llama_model_load_internal: format = ggjt v1 (pre #1405) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 128 llama_model_load_internal: n_embd = 4096 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 32 llama_model_load_internal: n_layer = 32 llama_model_load_internal: n_rot = 128 llama_model_load_internal: ftype = 0 (all F32) llama_model_load_internal: n_ff = 11008 llama_model_load_internal: n_parts = 1 llama_model_load_internal: model size = 7B llama_model_load_internal: ggml ctx size = 68.20 KB llama_model_load_internal: mem required = 27497.08 MB (+ 2052.00 MB per state) llama_init_from_file: kv self size = 128.00 MB The model loaded successfully. \u0026gt;\u0026gt;\u0026gt; What is the fastest programming language? Sending what is the fastest programming language? by Cary R on Jul 18, 2017, at 6:45 UTC what do you think it is and why? I\u0026#39;m not sure what \u0026#34;fast\u0026#34; means for this llama_print_timings: load time = 99393.39 ms llama_print_timings: sample time = 32.26 ms / 45 runs ( 0.72 ms per token) llama_print_timings: prompt eval time = 5021.80 ms / 10 tokens ( 502.18 ms per token) llama_print_timings: eval time = 15193.51 ms / 44 runs ( 345.31 ms per token) llama_print_timings: total time = 115311.61 ms Embeddings: [1.3335894 -0.83280444 0.9414267 -9.215284 -1.0302917 1.065452 -0.4542901 -0.24896632 -0.6570409 1.9119468 0.6292349 -0.14391524 0.2595427 -0.5855895 -0.963376 1.0406973 -0.1605502 1.3280734 0.37920082 0.61060756 -1.2766573 -1.8673204 1.2690753 -0.4294657 0.5546539 0.11715727 0.6430202 -0.09789314 -0.45095867 -1.1076287 0.042604066 0.15544033 -0.09977249 -1.3832492 0.018180523 2.2709634 0.26105422 -1.0794421 0.28251836 -1.2772827 1.3353819 -1.1416842 1.8800831 0.7737296 0.8329498 -1.1428409 -0.27773026 0.59615296 -1.1754322 -0.61925936 0.12707934 0.33790576 0.9590525 -1.0039365 1.2138838 -0.15244572 1.3892341 -0.2408304 -0.41973415 -0.9122008 0.61534476 -1.3473209 1.8957467 0.54428715 -0.45334002 -0.46586785 0.9365548 0.7735351 0.020367475 0.03640651 0.6072077 0.2598248 -0.60497457 0.74164164 -1.4986299 0.030030286 1.0310581 -0.7985864 0.59369475 5.3009334 -0.26436043 -1.0086688 0.69724923 -0.082101144 0.609409 -0.4504542 -0.57361007 -0.43234673 -0.621053 -1.3142335 -1.2885888 -0.29704484 0.16729134 -0.76317424 1.2080128 0.24425012 -0.3169634 0.9270621 1.0773871 -0.09211676 4.2189116 1.1267253 -1.2751623 -0.04176733 -1.0876625 -0.19441187 0.6124146 -0.5224489 -1.346519 -0.129513 -0.12585206 0.9263705 -1.6089619 -1.5251873 1.0640423 1.1027105 -0.5490974 -0.85569364 -1.1080054 0.9023686 -1.0494307 -0.28588632 -0.4288576 -0.72663045 1.7789608 2.239715 0.8199781 0.4134441] Memory usage : 110 MB CPU usage : 100%\nOk we have something - 345 ms per token. Makes sense, since it is an indirect non-native form of calling what is the first test of cpp\u0026hellip; except as a shared library via Golang. So what about natives?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ./llama.go --model ~/models/llama-7b-fp32.bin --prompt \u0026#34;Which programming language is fastest?\u0026#34; --context 45 --predict 45 /▒▒ /▒▒ /▒▒▒/▒▒▒ /▒▒/▒▒▒▒/▒▒ /▒▒▒/▒▒▒ /▒▒▒▒/▒▒ /▒▒▒/▒▒▒ /▒▒▒ /▒▒▒ /▒▒▒/ /▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒/ /▒▒▒ /▒▒▒▒ // /▒▒▒▒//▒▒▒ /▒▒▒▒/▒▒ /▒▒▒▒/▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒▒ /▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒ /▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒▒▒/▒▒ //▒▒▒/▒▒▒ //// /// //// /// /// //// /// // /// /// //// /// //// // /// /// ▒▒▒▒ [ LLaMA.go v1.4.0 ] [ LLaMA GPT in pure Golang - based on LLaMA C++ ] ▒▒▒▒ [ INIT ] Loading model, please wait ............................. [ PROMPT ] Which programming language is the fastest? [ OUTPUT ] Which programming language is best for a particular task? Which programming language should I use to write my program? What are the advantages and disadvantages of using a particular programming language? Comment: @ === EVAL TIME | ms === 15247 | 3125 | 1742 | 3636 | 1253 | 1276 | 1420 | 1487 | 1394 | 1628 | 1659 | 5330 | 10200 | 5287 | 3114 | 1320 | 1560 | 1725 | 5190 | 1573 | 1342 | 1440 | 1365 | 3922 | 13103 | 12436 | 3441 | 1597 | 1598 | 1537 | 1487 | 8702 | 3437 | 1780 | 10625 | 15753 | 1869 | 29020 | 7898 | 1574 | 1655 | 1664 | 1652 | 1604 | 1483 | === SAMPLING TIME | ms === 7 | 7 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 8 | 7 | 7 | 7 | 7 | 8 | 7 | 7 | 8 | 7 | 7 | 7 | 7 | 7 | 7 | 8 | 7 | 7 | 7 | === FULL TIME | ms === 0 | 15255 | 3132 | 1751 | 3643 | 1260 | 1283 | 1427 | 1495 | 1402 | 1635 | 1667 | 5339 | 10208 | 5295 | 3121 | 1328 | 1568 | 1732 | 5198 | 1581 | 1350 | 1448 | 1372 | 3930 | 13111 | 12444 | 3449 | 1605 | 1606 | 1545 | 1495 | 8710 | 3445 | 1788 | 10633 | 15761 | 1876 | 29028 | 7906 | 1581 | 1663 | 1673 | 1660 | 1612 | 1490 | [ HALT ] Time per token: 4380 ms | Tokens per second: 0.23 Memory usage : 25 GB CPU usage : 85%\nLooks like it loaded the entire model into RAM! ok\u0026hellip; well does that translate into better performance? Not good. I can tell you it felt slow, 4380ms seems \u0026hellip; accurate. Hmm\u0026hellip; maybe I should use the neon flag for my m1.\nTesting neon first:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ./llama.go --model ~/models/llama-7b-fp32.bin --prompt \u0026#34;Which programming language is fastest?\u0026#34; --neon --context 45 --predict 45 /▒▒ /▒▒ /▒▒▒/▒▒▒ /▒▒/▒▒▒▒/▒▒ /▒▒▒/▒▒▒ /▒▒▒▒/▒▒ /▒▒▒/▒▒▒ /▒▒▒ /▒▒▒ /▒▒▒/ /▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒/ /▒▒▒ /▒▒▒▒ // /▒▒▒▒//▒▒▒ /▒▒▒▒/▒▒ /▒▒▒▒/▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒/▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒▒ /▒▒ /▒▒▒▒/▒▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒▒/▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒ /▒▒▒ /▒▒▒ /▒▒▒▒ /▒▒▒//▒▒▒▒/▒▒ //▒▒▒/▒▒▒ //// /// //// /// /// //// /// // /// /// //// /// //// // /// /// ▒▒▒▒ [ LLaMA.go v1.4.0 ] [ LLaMA GPT in pure Golang - based on LLaMA C++ ] ▒▒▒▒ [ INIT ] Loading model, please wait ............................. [ PROMPT ] Which programming language is the fastest? [ OUTPUT ] Which programming language is easiest to use? Which programming language is the best for beginners? Which programming language is the best for experienced programmers? Which programming language has the most potential? Which programming === EVAL TIME | ms === 12559 | 3044 | 310 | 307 | 355 | 360 | 309 | 324 | 305 | 318 | 324 | 329 | 328 | 313 | 306 | 319 | 315 | 317 | 314 | 320 | 331 | 315 | 321 | 313 | 314 | 324 | 335 | 342 | 325 | 312 | 321 | 315 | 309 | 330 | 307 | 324 | 343 | 5317 | 312 | 312 | 307 | 319 | 323 | 316 | 309 | === SAMPLING TIME | ms === 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | === FULL TIME | ms === 0 | 12567 | 3051 | 318 | 315 | 363 | 367 | 316 | 331 | 313 | 326 | 331 | 337 | 336 | 321 | 313 | 326 | 323 | 325 | 321 | 328 | 339 | 322 | 328 | 321 | 322 | 331 | 342 | 350 | 333 | 319 | 329 | 323 | 316 | 338 | 314 | 332 | 350 | 5325 | 320 | 320 | 315 | 327 | 331 | 324 | 316 | [ HALT ] Time per token: 754 ms | Tokens per second: 1.33 Memory usage : 25GB CPU usage : 66%\nThe execution time significantly improved with the latest changes! The reported duration stands at approximately 754 ms per token. However, upon closer examination, it becomes evident that the initial token took a whopping 40 times longer to process, while the second token took 10 times longer compared to the rest. This disparity skews the average calculation, and a more realistic estimate would be around 315 ms per token, which aligns better with the perceived speed.\nNonetheless, it is worth noting that the program still has room for further optimization, as it did not fully utilize the available CPU resources. It is disappointing to observe that the current implementation falls short in this regard. Additionally, the necessity to load the model into RAM remains a drawback, especially when compared to the native C++ version, which accomplishes the same task with minimal RAM usage and greater efficiency.\nWhile the recent improvements have led to a noticeable boost in performance, there is still potential for even better optimizations. The requirement to load the model into RAM remains a limitation that hampers efficiency, particularly when compared to the native C++ version\u0026rsquo;s streamlined approach.\nLastly, is there any way we can use 100% of the CPU and squeeze out better performance? let\u0026rsquo;s try.\nusing --neon --context 45 --predict 45 --threads 10 --silent --profile 2023/05/14 10:25:56 profile: CPU profiling enabled, cpu.pprof I get 65% usage with 675 ms. 20 threads get 772ms 6 gave 50% usage with 745 ms per token 4 gave 35% usage with 915 ms per token 2 gave 20% usage with 1608 ms per token During the course of testing, it became evident that all the runs exhibited a similar pattern. The initial two tokens took an exceptionally long time to process, but subsequently, the performance noticeably improved. In a hypothetical scenario where superior optimizations were implemented, it is plausible that the native Go version could outperform the native C++ version, particularly if it efficiently utilized all CPU threads through goroutines. However, it must be acknowledged that the current implementation falls short in terms of performance. Additionally, one notable disadvantage of the native Go version is its inability to load the model in smaller segments, as the native C++ version does, thereby avoiding the excessive consumption of 25GB of RAM.\nIn summary, although the native Go version has the potential for faster performance through effective CPU thread utilization, it currently lags behind due to performance limitations. Furthermore, it lacks the advantageous feature present in the native C++ version of loading the model in smaller, more memory-efficient chunks.\nOk, that\u0026rsquo;s enough of that.\nBonus round The llama.cpp repo released GPU support for the program - so I had to try that out too. I don\u0026rsquo;t have anything too beefy to test it on , but I have a 1050 GPU laptop. So I went there and tested with GPU support enabled:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 .\\main.exe -m C:\\Users\\graha\\OneDrive\\Desktop\\llama-7b-fp32.bin -p \u0026#34;what is a banana doing on my lawn?\u0026#34; -t 12 -ngl 4 -c 45 -n 45 main: build = 550 (79b2d5b) main: seed = 1684092347 llama.cpp: loading model from C:\\Users\\graha\\OneDrive\\Desktop\\llama-7b-fp32.bin llama_model_load_internal: format = ggjt v1 (pre #1405) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 45 llama_model_load_internal: n_embd = 4096 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 32 llama_model_load_internal: n_layer = 32 llama_model_load_internal: n_rot = 128 llama_model_load_internal: ftype = 0 (all F32) llama_model_load_internal: n_ff = 11008 llama_model_load_internal: n_parts = 1 llama_model_load_internal: model size = 7B llama_model_load_internal: ggml ctx size = 72.75 KB llama_model_load_internal: mem required = 27497.09 MB (+ 1026.00 MB per state) llama_model_load_internal: [cublas] offloading 4 layers to GPU llama_model_load_internal: [cublas] total VRAM used: 3088 MB llama_init_from_file: kv self size = 22.50 MB system_info: n_threads = 12 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000 generate: n_ctx = 45, n_batch = 512, n_predict = 45, n_keep = 0 what is a banana doing on my lawn? the other day i was looking out the window and saw a banana sitting in our front yard. i walked outside to see if anyone had left it there by accident, but no one answered when i called for them inside llama_print_timings: load time = 35609.62 ms llama_print_timings: sample time = 10.52 ms / 45 runs ( 0.23 ms per token) llama_print_timings: prompt eval time = 31931.20 ms / 35 tokens ( 912.32 ms per token) llama_print_timings: eval time = 36327.48 ms / 43 runs ( 844.83 ms per token) llama_print_timings: total time = 74111.62 ms CPU usage : 100% memory usage : 25GB\nHmm, okay, so GPU support doesn\u0026rsquo;t provide significant benefits apart from offloading some of the RAM usage, resulting in a token processing time of approximately 844 ms (which is similar to the non-GPU version). Interestingly, the MacBook optimized code doesn\u0026rsquo;t utilize any RAM at all. Therefore, even if you possess a powerful GPU capable of efficient processing, it seems unlikely that it would greatly enhance the performance of this particular version of the llama program. Nevertheless, it\u0026rsquo;s still fascinating to observe!\nConclusion What have we learned from this analysis? Optimizations play a vital role in programming, the choice of programming language can significantly impact performance, and Python can be complex to configure for low-level operations. Due to these reasons, I refrained from making a direct comparison in this regard.\n","date":"2023-05-14T00:00:00Z","permalink":"/blog/posts/2023/may/llama-cpp-tests/","title":"My experience benchmarking llama"},{"content":"Disclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nPart 1 I will start by saying, I am a fan of Powershell. Even though it doesn\u0026rsquo;t fit the need for 90% of use cases I prefer it over bash for 3 reasons:\nIt\u0026rsquo;s object-oriented Almost all commands follow the same naming convention Arguments also follow a logical naming scheme. Consider in bash what will -f do for any random command. What about -c? Always completely different. The only consistent one may be v for verbose. I decided to compare the two. I will grade with the following attributes of each in mind:\nPerformance Syntax Consistency each attribute will get a score on a scale from 1-3\n1 2 3 1 = poor 2 = average or no difference 3 = superior But first a bit of background for context.\nHistory bash The Bourne Again SHell \u0026ndash; or bash was created in 1988 and ultimately released as a product by the \u0026rsquo;90s. The current version of bash is 5.1 (2020), which is not much different syntactically from version 3.0 released in 2004 around the time Powershell was being developed.\nPowerShell Driven by a lack of a cohesive scripting language needed for modern automation on Windows, Microsoft created Powershell in 2005. Eventually, Powershell v1.0 was out of beta in 2006. Shortly after, PowerShell v2.0 was released in 2008.\nBefore we begin My laptop:\n10th gen intel Windows 10 with bash using WSL Powershell prompt style: bash prompt style: Testing commands and functions I will test 15 different operations on each and judge each on the attributes listed above. Here are the 15 operations I have chosen to compare:\nPart 1:\nMake the directory Download the file write file size to the console find the file and unzip it find a string in any file count file sizes of a type Get the process by name and append it to a new file Ready? Go! Test 1: Make a Directory I will create test directories for each shell\nBash Command: mkdir -p bash/test{1..15}\nPowershell Command new-item -itemType directory $(1..15 | foreach{\u0026quot;powershell/test$_\u0026quot;})\nScore Stat Powershell Bash Performance 2 2 Syntax 1 3 Consistency 2 2 execution time 16.7 ms 11 ms \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 6 6 Winner: Tie\nTest 2: Download the zip form online Bash Command: wget https://github.com/mongodb/mongodb-kubernetes-operator/archive/refs/heads/master.zip\nPowershell Command : Invoke-WebRequest https://github.com/mongodb/mongodb-kubernetes-operator/archive/refs/heads/master.zip -outfile master.zip\nScore Stat Powershell Bash Performance 1 3 Syntax 1 3 Consistency 2 2 execution time 1.724 s 19.364 s \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 4 8 Winner: Bash\nTest 3: Write file size to console A simple example where you want to list a file\u0026rsquo;s size in the console\nBash Command : echo \u0026quot;Size : $(ls -lh master.zip|awk '{print $5}')\u0026quot;\nPowershell Command : write-host \u0026quot;Size: $((Get-Item master.zip).length/1KB)K\u0026quot;\nStat Powershell Bash Performance 2 2 Syntax 3 2 Consistency 3 3 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 8 7 Winner: Powershell\nTest 4: Find a file and unzip it Bash Command: find $(pwd) -name *.zip -exec unzip -q {} \\;\nPowershell Command: gci -name *.zip|foreach{Expand-Archive $_}\nScore Stat Powershell Bash Performance 1 3 Syntax 3 1 Consistency 3 1 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 7 5 Winner: Powershell\nTest 5: Find a string in any file Bash Command: grep -R testing\nPowershell Command: gci -Recurse | Select-String \u0026quot;testing\u0026quot; -List\nScore Stat Powershell Bash Performance 2 3 Syntax 2 3 Consistency 3 2 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 7 8 Winner: Bash\nTest 6: Count file sizes of a type Bash Script:\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash files=$(find . -name *.go) size=0 for i in $files do val=$(ls -l $i|awk \u0026#39;{print $5}\u0026#39;) size=$(( $size + $val )) done echo \u0026#34;Total goLang file size:\u0026#34; echo \u0026#34;Bytes: $size\u0026#34; echo \u0026#34;Kilobytes: $(($size / 1000))\u0026#34; TotalMilliseconds: 1879\nPowershell Script:\n1 2 3 4 5 6 $files=(gci -recurse *.go) $size = ($files | Measure-Object -Sum Length).Sum write-host \u0026#34;Total goLang file size:\u0026#34; write-host \u0026#34;Bytes: $size\u0026#34; write-host \u0026#34;Kilobytes: $($size/1KB)\u0026#34; TotalMilliseconds: 80.0936\nScore Stat Powershell Bash Performance 3 1 Syntax 3 1 Consistency 3 2 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 9 4 Winner: Powershell\nTest 7: Get the process by name and append it to a new file Bash Command: pgrep -f bash \u0026gt; out-file.txt \u0026amp;\u0026amp; wc out-file.txt\nPowershell Command : (get-process -name system).id|out-file out-file.txt; gci out-file.txt|Measure-Object –Line\nScore Stat Powershell Bash Performance 3 2 Syntax 2 2 Consistency 2 2 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 7 6 Winner: Powershell\nSummary Aggregate total for first 7 tests:\nStat Powershell Bash Performance 15 15 Syntax 16 15 Consistency 18 14 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;- Total 49 44 Powershell Feature spotlight: Advanced help pages. Powershell has an intuitive way to find commands that you may want to use. Unlike man pages on bash, which require you to know and read the exact binary that you are needing help with. Powershell allows for searching all help pages, for example:\nGet-Help -Name remoting\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Name Category Module Synopsis Add-AppvClientConnectionGroup Cmdlet AppvClient Add-AppvClientConnectionGroup... Get-AppvClientConnectionGroup Cmdlet AppvClient Get-AppvClientConnectionGroup... Enable-AppvClientConnectionGroup Cmdlet AppvClient Enable-AppvClientConnectionGroup... Remove-AppvClientConnectionGroup Cmdlet AppvClient Remove-AppvClientConnectionGroup... Disable-AppvClientConnectionGroup Cmdlet AppvClient Disable-AppvClientConnectionGroup... Repair-AppvClientConnectionGroup Cmdlet AppvClient Repair-AppvClientConnectionGroup... Stop-AppvClientConnectionGroup Cmdlet AppvClient Stop-AppvClientConnectionGroup... Connect-VMNetworkAdapter Cmdlet Hyper-V Connect-VMNetworkAdapter... Grant-VMConnectAccess Cmdlet Hyper-V Grant-VMConnectAccess... Disconnect-VMNetworkAdapter Cmdlet Hyper-V Disconnect-VMNetworkAdapter... Get-VMConnectAccess Cmdlet Hyper-V Get-VMConnectAccess... Disconnect-VMSan Cmdlet Hyper-V Disconnect-VMSan... Revoke-VMConnectAccess Cmdlet Hyper-V Revoke-VMConnectAccess... Test-VMReplicationConnection Cmdlet Hyper-V Test-VMReplicationConnection... Connect-VMSan Cmdlet Hyper-V Connect-VMSan... Get-IscsiConnection Function iSCSI ... Disconnect-IscsiTarget Function iSCSI ... Connect-IscsiTarget Function iSCSI ... Disconnect-WSMan Cmdlet Microsoft.WSMan.Manage... Disconnect-WSMan... Connect-WSMan Cmdlet Microsoft.WSMan.Manage... Connect-WSMan... Get-NetConnectionProfile Function NetConnection ... Set-NetConnectionProfile Function NetConnection ... Get-NetTCPConnection Function NetTCPIP ... Test-NetConnection Function NetTCPIP ... Consistency All of Microsoft\u0026rsquo;s cmdlets have a Verb-Noun structure.\nAll of Microsoft\u0026rsquo;s cmdlets can easily be researched on Google because no other program calls their commands \u0026ldquo;cmdlets\u0026rdquo;. So, you will always get something on PowerShell if you use that word.\nAll functions are object-oriented, making scripting and automation work the same way on every command. No more guessing which column a specific value you are looking for is on like in bash. No more trimming whitespace and tab characters.\nAll loops automatically use the $_ notation for the object being looped over. So for example, every loop can reference $_.name to get the name property if it exists.\nBottom Line Powershell is fundamentally a more intuitive and powerful scripting language than bash. However, the most symmetrical comparison for Powershell would be something we didn\u0026rsquo;t compare: python.\nThere is little reason to use Powershell on Linux because it\u0026rsquo;s not often packaged with Linux. However, python is more powerful than bash and is your most likely target language on Linux for automation. The only caveat is that you would want to use bash for system calls and configuration, and then have Python specifically for automation. Whereas, on windows, PowerShell can do it all.\nIf you are using Windows and looking to script and automate, PowerShell is your friend.\n","date":"2023-05-13T00:00:00Z","permalink":"/blog/posts/2023/april/bash-vs-powershell/","title":"Bash Vs Powershell"},{"content":"An homage to Starwars fans everywhere (see original blog post for better experience)\n","date":"2023-05-04T00:00:00Z","permalink":"/blog/posts/2023/may/may-the-4th/","title":"May the 4th be with you"},{"content":"\nNote: This blog is mainly for development purposes.\nDisclosure: The words in this post were not AI-generated or altered in any meaningful way. Spell check and other tools were used, but all content and phrases are my own creation.\nSummary I have created this blogs website to accomplish a few things:\nInformation Sharing I wanted a place to create posts and information for my purposes, but also for public references (sharing with hyperlinks on websites), but have never had somewhere that was both easy to create and also purely my creation. Developer Creativity This website allows me to express my interests in certain technologies by posting about my discoveries, how-tos, and analysis. Personal Projects I also can experiment and create web-framework-related ideas with actual visible usage. About the site This website is a Github pages website, meaning its static content is served from my github blog repo. Since this repo is public you can take a look at how it works and maybe find inspiration yourself.\nI think it\u0026rsquo;s the best solution to what I am looking for. I can create posts easily by :\nAdd a new md post, which I have decided to put in a posts/ directory on GitHub. Update the schema.yml to update the static website to automatically add the page to the html/javascript website (no rebuild needed). That\u0026rsquo;s it! A new post! Since it\u0026rsquo;s on GitHub, it has history and version control. Another concern I had was putting my blog somewhere that would be forgotten or eventually discontinued. Since GitHub is trustworthy and reliable (it won\u0026rsquo;t suffer outages), this shouldn\u0026rsquo;t be an issue. I am also leveraging a free functionality of GitHub pages for basically zero-touch configuration.\nHowever, to be clear, I don\u0026rsquo;t simply use GitHub pages. I also host my website on https://gportal.link which is a website I have running on Vultr for $6 a month which contains an Nginx reverse proxy and a few hosted website components such as a golang authentication server. For my peace of mind, I keep that repo private! Sorry :)\nWhat about you? So, perhaps I will allow comments one day, but there\u0026rsquo;s not much interaction yet. Stay tuned!\nFinal thoughts Everything I create is in the spirit of learning the best way to accomplish functionality with the following criteria always in mind:\nChoose the simplest option that satisfies your requirements. Choose what will be easiest to maintaintain. Choose what allows configuration to meet your changing needs. This method almost missed the mark on the final bullet point, because solely using markdown can limit my formatting. But if I need more formatting in the future, I can use javascript via Svelte to do so.\nHowever you stumbled across this blog, I hope you learn something new and find it as fun as I do to create.\nHere\u0026rsquo;s a snapshot of what the blog looks like right now.. a bit sad. Hopefully better soon:\n","date":"2023-04-29T00:00:00Z","permalink":"/blog/posts/2023/april/first-post/","title":"First Post!"},{"content":"Posted Date: Sat May 13th, 2023\nWhy it matters I believe AI will disrupt society in exactly the same way other disruptive technologies were in the past. First it will get a lot of attention, before fading into the background of progress. Anyone preaching doom and gloom or \u0026ldquo;change like never before\u0026rdquo; are likely misguided or misinformed.\nAI is going to be an important part of society - but so was the advent of the telephone, sewer systems, refridgeration, the auto mobile, cement for construction, or the incondecent light bulb. All life changing technologies. So too will AI be life changing.\nThe debate about whether its ultimately good or bad is unimportant to me. Whether good or bad, we don\u0026rsquo;t know. But we can say its inevitable and will cause sweeping change.\n","date":"0001-01-01T00:00:00Z","permalink":"/blog/posts/2023/may/ai-in-the-news/","title":""},{"content":"Part 2: 8) find a file 9) list file content 10) find a string in a file 11) Make directory 12) find a file 13) list file content 14) find a string in a file\n","date":"0001-01-01T00:00:00Z","permalink":"/blog/posts/2023/may/bash-vs-powershell-2/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"/blog/posts/2023/may/my-infrastructure/","title":""}]